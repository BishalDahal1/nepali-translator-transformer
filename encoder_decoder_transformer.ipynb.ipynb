{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b08602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: MPS (Apple Silicon M3 GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import sentencepiece as spm  \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using Device: CUDA (NVIDIA GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Device: MPS (Apple Silicon M3 GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using Device: CPU (Slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ce48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83916940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nepali tokenizer already exists.\n",
      "English tokenizer already exists.\n"
     ]
    }
   ],
   "source": [
    "en_filepath = 'en-ne.txt/OpenSubtitles.en-ne.en'\n",
    "ne_filepath = 'en-ne.txt/OpenSubtitles.en-ne.ne'\n",
    "\n",
    "if not os.path.exists('nepali_bpe.model'):\n",
    "    print(\"Training Nepali Tokenizer...\")\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=ne_filepath, \n",
    "        model_prefix='nepali_bpe', \n",
    "        vocab_size=8000, \n",
    "        model_type='bpe',\n",
    "        pad_id=0, bos_id=1, eos_id=2, unk_id=3, \n",
    "        character_coverage=1.0\n",
    "    )\n",
    "else:\n",
    "    print(\"Nepali tokenizer already exists.\")\n",
    "\n",
    "if not os.path.exists('english_bpe.model'):\n",
    "    print(\"Training English Tokenizer...\")\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=en_filepath, \n",
    "        model_prefix='english_bpe', \n",
    "        vocab_size=8000, \n",
    "        model_type='bpe',\n",
    "        pad_id=0, bos_id=1, eos_id=2, unk_id=3, \n",
    "        character_coverage=1.0\n",
    "    )\n",
    "else:\n",
    "    print(\"English tokenizer already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b981638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading en-ne.txt/OpenSubtitles.en-ne.en...\n",
      "Reading en-ne.txt/OpenSubtitles.en-ne.ne...\n",
      "Successfully loaded 149006 sentence pairs.\n"
     ]
    }
   ],
   "source": [
    "sp_en = spm.SentencePieceProcessor(model_file='english_bpe.model')\n",
    "sp_ne = spm.SentencePieceProcessor(model_file='nepali_bpe.model')\n",
    "\n",
    "def read_file(filename):\n",
    "    print(f\"Reading {filename}...\")\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "try:\n",
    "    english_sentences = read_file(en_filepath)\n",
    "    nepali_sentences = read_file(ne_filepath)\n",
    "    print(f\"Successfully loaded {len(english_sentences)} sentence pairs.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: File not found. Check if you are in the 'Translation_Model' folder.\")\n",
    "\n",
    "class EnNeDataset(Dataset):\n",
    "    def __init__(self, en_sentences, ne_sentences, sp_en, sp_ne, max_len=128):\n",
    "        self.en_sentences = en_sentences\n",
    "        self.ne_sentences = ne_sentences\n",
    "        self.sp_en = sp_en\n",
    "        self.sp_ne = sp_ne\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        en_text = self.en_sentences[idx]\n",
    "        ne_text = self.ne_sentences[idx]\n",
    "        \n",
    "        en_encoded = [1] + self.sp_en.encode_as_ids(en_text) + [2]\n",
    "        ne_encoded = [1] + self.sp_ne.encode_as_ids(ne_text) + [2]\n",
    "        \n",
    "        if len(en_encoded) > self.max_len: en_encoded = en_encoded[:self.max_len]\n",
    "        if len(ne_encoded) > self.max_len: ne_encoded = ne_encoded[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(en_encoded), torch.tensor(ne_encoded)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f0d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training!\n",
      "Training Batches: 4191\n",
      "Validation Batches: 466\n",
      "Sample Batch Shape - Source: torch.Size([32, 16]), Target: torch.Size([32, 15])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "full_dataset = EnNeDataset(english_sentences, nepali_sentences, sp_en, sp_ne)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Ready for training!\")\n",
    "print(f\"Training Batches: {len(train_loader)}\")\n",
    "print(f\"Validation Batches: {len(valid_loader)}\")\n",
    "\n",
    "src_sample, trg_sample = next(iter(train_loader))\n",
    "print(f\"Sample Batch Shape - Source: {src_sample.shape}, Target: {trg_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803d87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40c2711",
   "metadata": {},
   "source": [
    "<b>ENCODER</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inp_dim,\n",
    "                 hid_dim,\n",
    "                 pf_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 drop_out,\n",
    "                 device,\n",
    "                 block_size = 100):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(inp_dim,hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(block_size,hid_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  drop_out,\n",
    "                                                  device\n",
    "                                                  ) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, src , src_mask):\n",
    "        batch_size = src.shape[0] \n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        # src = [batch_size , src_len]\n",
    "        #src_mask = [.., 1 , 1 , ...]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch_size , src_len]\n",
    "\n",
    "        src = self.dropout(self.tok_embedding(src) * self.scale + self.pos_embedding(pos)) \n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        #src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 drop_out, \n",
    "                 device ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attentation = MultiHeadSelfAttentation(hid_dim, n_heads , drop_out , device)\n",
    "        self.feedforward = FeedForward(hid_dim, pf_dim, drop_out)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, src , src_mask):\n",
    "\n",
    "        # src = [batch_size , src_len , hid_dim]\n",
    "        #src_mask = [.., 1 , 1 , src_len]\n",
    "\n",
    "        _src, _ = self.self_attentation(src, src, src , src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch_size , src_len , hid_dim]\n",
    "         \n",
    "        _src = self.feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch_size , src_len , hid_dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b903d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentation(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "\n",
    "        self.key = nn.Linear(hid_dim, hid_dim , bias=False)\n",
    "        self.query = nn.Linear(hid_dim, hid_dim , bias=False)\n",
    "        self.value = nn.Linear(hid_dim, hid_dim , bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(hid_dim, hid_dim , bias=False)\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, q ,k , v , mask = None):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        K = self.key(k) \n",
    "        Q = self.query(q) \n",
    "        V = self.value(v) # [batch , block(len) , hid_dim]\n",
    "\n",
    "        K = K.view(batch_size , -1 , self.n_heads , self.head_dim).permute(0, 2 ,1 , 3) \n",
    "        Q = Q.view(batch_size , -1 , self.n_heads , self.head_dim).permute(0, 2 ,1 , 3)\n",
    "        V = V.view(batch_size , -1 , self.n_heads , self.head_dim).permute(0, 2 ,1 , 3) #[batch , n_heads , block , head_size]\n",
    "\n",
    "        wei = Q @ K.permute(0 , 1, 3, 2) * self.scale #* self.hid_dim**-0.5\n",
    "        if mask is not None :\n",
    "            # tril = torch.tril(torch.ones(q.shape[1], q.shape[1]))\n",
    "            wei = wei.masked_fill(mask == 0 , float('-inf'))\n",
    "\n",
    "        wei = F.softmax(wei , dim=-1) \n",
    "        wei = self.dropout(wei)\n",
    "        x = wei @ V # V = [batch , n_heads , block , head_size] , wei = [batch size, n_heads, trg_len, src_len]\n",
    "\n",
    "        #x = [batch , n_heads , block , head_size]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        #x = [batch , block , n_heads , head_size]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        #X = [batch , block(len) , hid_dim = n_heads * head_size]\n",
    "\n",
    "\n",
    "        x = self.proj(x)\n",
    "        #X = [batch , block(len) , hid_dim]\n",
    "        return x, wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self , hid_dim , pf_dim , dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hid_dim,pf_dim), #[batch , block(len) , pf_dim]\n",
    "            nn.ReLU(), #avoid vanishing grading problem \n",
    "            nn.Linear(pf_dim, hid_dim), # [batch , block(len) , hid_dim]\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce711572",
   "metadata": {},
   "source": [
    "<b>Decoder</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_dim,\n",
    "                 hid_dim,\n",
    "                 pf_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 drop_out,\n",
    "                 device,\n",
    "                 block_size = 100):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(out_dim,hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(block_size,hid_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  drop_out,\n",
    "                                                  device\n",
    "                                                  ) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.proj = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, trg , src_enc , trg_mask , src_mask):\n",
    "\n",
    "        #trg = [B , T]\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) #[B,T]\n",
    "\n",
    "        trg = self.dropout(self.tok_embedding(trg) * self.scale + self.pos_embedding(pos)) #[B,T,hid_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attetation = layer(trg , src_enc , trg_mask , src_mask)\n",
    "        output = self.proj(trg) #[B,T,out_dim]\n",
    "\n",
    "        return output, attetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 drop_out, \n",
    "                 device ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attentation = MultiHeadSelfAttentation(hid_dim, n_heads , drop_out , device)\n",
    "        self.enc_attentation = MultiHeadSelfAttentation(hid_dim, n_heads , drop_out , device)\n",
    "        self.feedforward = FeedForward(hid_dim, pf_dim, drop_out)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, trg , src_enc , trg_mask , src_mask):\n",
    "\n",
    "        #trg = [batch size, trg_len, hid dim]]\n",
    "        #src_enc = [batch size, src_len, hid dim]]\n",
    "        #trg_mask = [batch_size , 1 , trg_len , block_size]\n",
    "\n",
    "        _trg, _ = self.self_attentation(trg, trg , trg ,trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "\n",
    "        _trg, attention = self.enc_attentation(trg, src_enc, src_enc, src_mask)\n",
    "\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "         \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        #trg = [batch size, trg_len, hid dim]\n",
    "        #attention = [batch size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg , attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b07eb6",
   "metadata": {},
   "source": [
    "<b>Seq2Seq</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "    #src = [batch_size , block_size]\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2) # shape = [batch_size , 1 , 1, block_size ]\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        #trg = [batch_size, block_size]\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #trg_pad_mask = [batch_size, 1 ,1 , block_size]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool() #[trg_len, trg_len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask #[batch_size , 1 , trg_len , block_size]\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self,src , trg ):\n",
    "        src_mask = self.make_src_mask(src) #[batch_size, 1 ,1 , block_size]\n",
    "\n",
    "        src_enc = self.encoder(src, src_mask) #[batch_size, block(len), hid_dim]\n",
    "\n",
    "        trg_mask = self.make_trg_mask(trg) #[batch_size , 1 , trg_len , block_size]\n",
    "\n",
    "        output , attentation = self.decoder(trg ,src_enc , trg_mask, src_mask) \n",
    "\n",
    "        return output, attentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8598c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = sp_en.get_piece_size()\n",
    "OUTPUT_DIM = sp_ne.get_piece_size()\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, HID_DIM, ENC_PF_DIM, ENC_LAYERS, ENC_HEADS, ENC_DROPOUT, device , block_size=200)\n",
    "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_PF_DIM, DEC_LAYERS, DEC_HEADS, DEC_DROPOUT, device, block_size=200)\n",
    "\n",
    "SRC_PAD_IDX = 0\n",
    "TRG_PAD_IDX = 0\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d85fe5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,198,848 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12741ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47213ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b11e1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "LEARNING_RATE = 0.0001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfd37896",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96387554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator , optimizer , criterion , clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0 \n",
    "\n",
    "    for i, (src,trg) in enumerate(iterator):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output , _ = model(src , trg[:, :-1]) \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Batch {i+1}/{len(iterator)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1b9bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model , iterator , criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5740d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving initial model weights...\n",
      "✅ Initial model saved to 'models/model_init.pt'\n",
      "\n",
      "==================================================\n",
      "Starting Training...\n",
      "==================================================\n",
      "\n",
      "  Batch 100/4191 | Loss: 4.5825\n",
      "  Batch 200/4191 | Loss: 4.2848\n",
      "  Batch 300/4191 | Loss: 4.5157\n",
      "  Batch 400/4191 | Loss: 4.3945\n",
      "  Batch 500/4191 | Loss: 4.1056\n",
      "  Batch 600/4191 | Loss: 4.2638\n",
      "  Batch 700/4191 | Loss: 3.9948\n",
      "  Batch 800/4191 | Loss: 4.4187\n",
      "  Batch 900/4191 | Loss: 3.9116\n",
      "  Batch 1000/4191 | Loss: 4.3112\n",
      "  Batch 1100/4191 | Loss: 4.5482\n",
      "  Batch 1200/4191 | Loss: 4.0666\n",
      "  Batch 1300/4191 | Loss: 4.3216\n",
      "  Batch 1400/4191 | Loss: 3.9779\n",
      "  Batch 1500/4191 | Loss: 4.1606\n",
      "  Batch 1600/4191 | Loss: 4.6175\n",
      "  Batch 1700/4191 | Loss: 3.7342\n",
      "  Batch 1800/4191 | Loss: 4.3024\n",
      "  Batch 1900/4191 | Loss: 4.6097\n",
      "  Batch 2000/4191 | Loss: 4.4754\n",
      "  Batch 2100/4191 | Loss: 4.6334\n",
      "  Batch 2200/4191 | Loss: 4.8746\n",
      "  Batch 2300/4191 | Loss: 4.2754\n",
      "  Batch 2400/4191 | Loss: 4.5133\n",
      "  Batch 2500/4191 | Loss: 3.9831\n",
      "  Batch 2600/4191 | Loss: 4.3009\n",
      "  Batch 2700/4191 | Loss: 4.2630\n",
      "  Batch 2800/4191 | Loss: 4.0329\n",
      "  Batch 2900/4191 | Loss: 4.2573\n",
      "  Batch 3000/4191 | Loss: 4.5500\n",
      "  Batch 3100/4191 | Loss: 4.2549\n",
      "  Batch 3200/4191 | Loss: 4.1203\n",
      "  Batch 3300/4191 | Loss: 4.3669\n",
      "  Batch 3400/4191 | Loss: 4.3530\n",
      "  Batch 3500/4191 | Loss: 4.3010\n",
      "  Batch 3600/4191 | Loss: 4.7679\n",
      "  Batch 3700/4191 | Loss: 3.6647\n",
      "  Batch 3800/4191 | Loss: 3.9577\n",
      "  Batch 3900/4191 | Loss: 4.2489\n",
      "  Batch 4000/4191 | Loss: 4.0549\n",
      "  Batch 4100/4191 | Loss: 4.0096\n",
      "\n",
      "Epoch: 01 | Time: 4m 7s\n",
      "\tTrain Loss: 4.295 | Train PPL:  73.308\n",
      "\t Val. Loss: 4.337 |  Val. PPL:  76.474 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 4.0895\n",
      "  Batch 200/4191 | Loss: 3.8513\n",
      "  Batch 300/4191 | Loss: 4.3723\n",
      "  Batch 400/4191 | Loss: 4.4978\n",
      "  Batch 500/4191 | Loss: 4.6424\n",
      "  Batch 600/4191 | Loss: 4.5050\n",
      "  Batch 700/4191 | Loss: 4.2185\n",
      "  Batch 800/4191 | Loss: 4.3008\n",
      "  Batch 900/4191 | Loss: 4.0795\n",
      "  Batch 1000/4191 | Loss: 4.2002\n",
      "  Batch 1100/4191 | Loss: 3.8965\n",
      "  Batch 1200/4191 | Loss: 4.4508\n",
      "  Batch 1300/4191 | Loss: 4.3365\n",
      "  Batch 1400/4191 | Loss: 4.3667\n",
      "  Batch 1500/4191 | Loss: 4.1419\n",
      "  Batch 1600/4191 | Loss: 4.3165\n",
      "  Batch 1700/4191 | Loss: 4.4358\n",
      "  Batch 1800/4191 | Loss: 4.5339\n",
      "  Batch 1900/4191 | Loss: 4.7074\n",
      "  Batch 2000/4191 | Loss: 4.3052\n",
      "  Batch 2100/4191 | Loss: 4.0441\n",
      "  Batch 2200/4191 | Loss: 4.4422\n",
      "  Batch 2300/4191 | Loss: 4.1220\n",
      "  Batch 2400/4191 | Loss: 4.1303\n",
      "  Batch 2500/4191 | Loss: 4.0779\n",
      "  Batch 2600/4191 | Loss: 4.0835\n",
      "  Batch 2700/4191 | Loss: 4.4283\n",
      "  Batch 2800/4191 | Loss: 4.7377\n",
      "  Batch 2900/4191 | Loss: 4.8067\n",
      "  Batch 3000/4191 | Loss: 3.7598\n",
      "  Batch 3100/4191 | Loss: 4.5760\n",
      "  Batch 3200/4191 | Loss: 4.1802\n",
      "  Batch 3300/4191 | Loss: 4.2254\n",
      "  Batch 3400/4191 | Loss: 4.0398\n",
      "  Batch 3500/4191 | Loss: 4.1788\n",
      "  Batch 3600/4191 | Loss: 4.0738\n",
      "  Batch 3700/4191 | Loss: 4.4863\n",
      "  Batch 3800/4191 | Loss: 4.3822\n",
      "  Batch 3900/4191 | Loss: 4.5685\n",
      "  Batch 4000/4191 | Loss: 3.7890\n",
      "  Batch 4100/4191 | Loss: 4.5024\n",
      "\n",
      "Epoch: 02 | Time: 4m 6s\n",
      "\tTrain Loss: 4.216 | Train PPL:  67.756\n",
      "\t Val. Loss: 4.285 |  Val. PPL:  72.632 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.8824\n",
      "  Batch 200/4191 | Loss: 4.5634\n",
      "  Batch 300/4191 | Loss: 4.1765\n",
      "  Batch 400/4191 | Loss: 3.9347\n",
      "  Batch 500/4191 | Loss: 4.0556\n",
      "  Batch 600/4191 | Loss: 3.9474\n",
      "  Batch 700/4191 | Loss: 4.1080\n",
      "  Batch 800/4191 | Loss: 4.2699\n",
      "  Batch 900/4191 | Loss: 3.9683\n",
      "  Batch 1000/4191 | Loss: 4.0029\n",
      "  Batch 1100/4191 | Loss: 4.1633\n",
      "  Batch 1200/4191 | Loss: 3.7270\n",
      "  Batch 1300/4191 | Loss: 4.3065\n",
      "  Batch 1400/4191 | Loss: 3.6221\n",
      "  Batch 1500/4191 | Loss: 4.1265\n",
      "  Batch 1600/4191 | Loss: 3.9822\n",
      "  Batch 1700/4191 | Loss: 4.3718\n",
      "  Batch 1800/4191 | Loss: 4.0838\n",
      "  Batch 1900/4191 | Loss: 4.3156\n",
      "  Batch 2000/4191 | Loss: 4.2778\n",
      "  Batch 2100/4191 | Loss: 4.1515\n",
      "  Batch 2200/4191 | Loss: 4.3454\n",
      "  Batch 2300/4191 | Loss: 3.7778\n",
      "  Batch 2400/4191 | Loss: 4.3056\n",
      "  Batch 2500/4191 | Loss: 4.2131\n",
      "  Batch 2600/4191 | Loss: 4.0369\n",
      "  Batch 2700/4191 | Loss: 3.6804\n",
      "  Batch 2800/4191 | Loss: 4.3919\n",
      "  Batch 2900/4191 | Loss: 4.2892\n",
      "  Batch 3000/4191 | Loss: 4.0513\n",
      "  Batch 3100/4191 | Loss: 4.2173\n",
      "  Batch 3200/4191 | Loss: 4.1522\n",
      "  Batch 3300/4191 | Loss: 4.2844\n",
      "  Batch 3400/4191 | Loss: 4.3457\n",
      "  Batch 3500/4191 | Loss: 4.4234\n",
      "  Batch 3600/4191 | Loss: 4.0448\n",
      "  Batch 3700/4191 | Loss: 3.9200\n",
      "  Batch 3800/4191 | Loss: 4.4906\n",
      "  Batch 3900/4191 | Loss: 4.2119\n",
      "  Batch 4000/4191 | Loss: 3.8878\n",
      "  Batch 4100/4191 | Loss: 4.5637\n",
      "\n",
      "Epoch: 03 | Time: 4m 15s\n",
      "\tTrain Loss: 4.148 | Train PPL:  63.301\n",
      "\t Val. Loss: 4.218 |  Val. PPL:  67.881 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 4.0831\n",
      "  Batch 200/4191 | Loss: 4.0353\n",
      "  Batch 300/4191 | Loss: 4.1165\n",
      "  Batch 400/4191 | Loss: 3.7282\n",
      "  Batch 500/4191 | Loss: 3.6349\n",
      "  Batch 600/4191 | Loss: 4.3827\n",
      "  Batch 700/4191 | Loss: 4.0531\n",
      "  Batch 800/4191 | Loss: 4.3359\n",
      "  Batch 900/4191 | Loss: 4.4856\n",
      "  Batch 1000/4191 | Loss: 4.5254\n",
      "  Batch 1100/4191 | Loss: 3.9170\n",
      "  Batch 1200/4191 | Loss: 3.8250\n",
      "  Batch 1300/4191 | Loss: 4.1243\n",
      "  Batch 1400/4191 | Loss: 4.2446\n",
      "  Batch 1500/4191 | Loss: 4.6034\n",
      "  Batch 1600/4191 | Loss: 4.1125\n",
      "  Batch 1700/4191 | Loss: 3.7172\n",
      "  Batch 1800/4191 | Loss: 3.8530\n",
      "  Batch 1900/4191 | Loss: 4.2953\n",
      "  Batch 2000/4191 | Loss: 4.1068\n",
      "  Batch 2100/4191 | Loss: 4.2101\n",
      "  Batch 2200/4191 | Loss: 3.9473\n",
      "  Batch 2300/4191 | Loss: 3.8620\n",
      "  Batch 2400/4191 | Loss: 4.2145\n",
      "  Batch 2500/4191 | Loss: 3.6483\n",
      "  Batch 2600/4191 | Loss: 4.1715\n",
      "  Batch 2700/4191 | Loss: 4.4383\n",
      "  Batch 2800/4191 | Loss: 3.9136\n",
      "  Batch 2900/4191 | Loss: 3.9817\n",
      "  Batch 3000/4191 | Loss: 4.1587\n",
      "  Batch 3100/4191 | Loss: 4.2122\n",
      "  Batch 3200/4191 | Loss: 3.9604\n",
      "  Batch 3300/4191 | Loss: 4.2723\n",
      "  Batch 3400/4191 | Loss: 3.9301\n",
      "  Batch 3500/4191 | Loss: 3.7843\n",
      "  Batch 3600/4191 | Loss: 4.3983\n",
      "  Batch 3700/4191 | Loss: 4.1052\n",
      "  Batch 3800/4191 | Loss: 3.9373\n",
      "  Batch 3900/4191 | Loss: 4.3474\n",
      "  Batch 4000/4191 | Loss: 4.3670\n",
      "  Batch 4100/4191 | Loss: 3.9136\n",
      "\n",
      "Epoch: 04 | Time: 4m 15s\n",
      "\tTrain Loss: 4.079 | Train PPL:  59.066\n",
      "\t Val. Loss: 4.188 |  Val. PPL:  65.911 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 4.2283\n",
      "  Batch 200/4191 | Loss: 4.0092\n",
      "  Batch 300/4191 | Loss: 4.4076\n",
      "  Batch 400/4191 | Loss: 3.9409\n",
      "  Batch 500/4191 | Loss: 3.9802\n",
      "  Batch 600/4191 | Loss: 4.4737\n",
      "  Batch 700/4191 | Loss: 4.2345\n",
      "  Batch 800/4191 | Loss: 4.1164\n",
      "  Batch 900/4191 | Loss: 3.7335\n",
      "  Batch 1000/4191 | Loss: 4.2025\n",
      "  Batch 1100/4191 | Loss: 3.2798\n",
      "  Batch 1200/4191 | Loss: 3.4076\n",
      "  Batch 1300/4191 | Loss: 4.1489\n",
      "  Batch 1400/4191 | Loss: 4.0413\n",
      "  Batch 1500/4191 | Loss: 3.6236\n",
      "  Batch 1600/4191 | Loss: 3.8096\n",
      "  Batch 1700/4191 | Loss: 4.2349\n",
      "  Batch 1800/4191 | Loss: 4.3381\n",
      "  Batch 1900/4191 | Loss: 3.9947\n",
      "  Batch 2000/4191 | Loss: 4.0218\n",
      "  Batch 2100/4191 | Loss: 4.1528\n",
      "  Batch 2200/4191 | Loss: 4.0969\n",
      "  Batch 2300/4191 | Loss: 4.1613\n",
      "  Batch 2400/4191 | Loss: 3.8312\n",
      "  Batch 2500/4191 | Loss: 3.7474\n",
      "  Batch 2600/4191 | Loss: 3.9588\n",
      "  Batch 2700/4191 | Loss: 3.9746\n",
      "  Batch 2800/4191 | Loss: 4.4222\n",
      "  Batch 2900/4191 | Loss: 3.9481\n",
      "  Batch 3000/4191 | Loss: 4.2577\n",
      "  Batch 3100/4191 | Loss: 3.9634\n",
      "  Batch 3200/4191 | Loss: 4.4667\n",
      "  Batch 3300/4191 | Loss: 4.0583\n",
      "  Batch 3400/4191 | Loss: 4.1130\n",
      "  Batch 3500/4191 | Loss: 4.4130\n",
      "  Batch 3600/4191 | Loss: 3.8825\n",
      "  Batch 3700/4191 | Loss: 4.0534\n",
      "  Batch 3800/4191 | Loss: 4.0100\n",
      "  Batch 3900/4191 | Loss: 4.0300\n",
      "  Batch 4000/4191 | Loss: 4.2892\n",
      "  Batch 4100/4191 | Loss: 3.6463\n",
      "\n",
      "Epoch: 05 | Time: 4m 19s\n",
      "\tTrain Loss: 4.007 | Train PPL:  54.999\n",
      "\t Val. Loss: 4.153 |  Val. PPL:  63.655 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.5898\n",
      "  Batch 200/4191 | Loss: 4.0051\n",
      "  Batch 300/4191 | Loss: 4.0842\n",
      "  Batch 400/4191 | Loss: 3.6064\n",
      "  Batch 500/4191 | Loss: 3.8796\n",
      "  Batch 600/4191 | Loss: 3.9269\n",
      "  Batch 700/4191 | Loss: 3.8446\n",
      "  Batch 800/4191 | Loss: 3.9554\n",
      "  Batch 900/4191 | Loss: 3.7568\n",
      "  Batch 1000/4191 | Loss: 4.1546\n",
      "  Batch 1100/4191 | Loss: 4.0518\n",
      "  Batch 1200/4191 | Loss: 4.5913\n",
      "  Batch 1300/4191 | Loss: 4.0276\n",
      "  Batch 1400/4191 | Loss: 4.1386\n",
      "  Batch 1500/4191 | Loss: 4.0167\n",
      "  Batch 1600/4191 | Loss: 4.0382\n",
      "  Batch 1700/4191 | Loss: 3.3707\n",
      "  Batch 1800/4191 | Loss: 4.0630\n",
      "  Batch 1900/4191 | Loss: 3.4203\n",
      "  Batch 2000/4191 | Loss: 3.7824\n",
      "  Batch 2100/4191 | Loss: 4.2179\n",
      "  Batch 2200/4191 | Loss: 3.8830\n",
      "  Batch 2300/4191 | Loss: 3.7227\n",
      "  Batch 2400/4191 | Loss: 4.0831\n",
      "  Batch 2500/4191 | Loss: 3.8564\n",
      "  Batch 2600/4191 | Loss: 3.6603\n",
      "  Batch 2700/4191 | Loss: 4.2483\n",
      "  Batch 2800/4191 | Loss: 4.0030\n",
      "  Batch 2900/4191 | Loss: 3.5690\n",
      "  Batch 3000/4191 | Loss: 4.1774\n",
      "  Batch 3100/4191 | Loss: 3.9849\n",
      "  Batch 3200/4191 | Loss: 4.0138\n",
      "  Batch 3300/4191 | Loss: 4.0608\n",
      "  Batch 3400/4191 | Loss: 4.1313\n",
      "  Batch 3500/4191 | Loss: 4.1612\n",
      "  Batch 3600/4191 | Loss: 4.0426\n",
      "  Batch 3700/4191 | Loss: 3.6538\n",
      "  Batch 3800/4191 | Loss: 3.8748\n",
      "  Batch 3900/4191 | Loss: 3.7850\n",
      "  Batch 4000/4191 | Loss: 4.0783\n",
      "  Batch 4100/4191 | Loss: 4.0681\n",
      "\n",
      "Epoch: 06 | Time: 4m 22s\n",
      "\tTrain Loss: 3.952 | Train PPL:  52.032\n",
      "\t Val. Loss: 4.117 |  Val. PPL:  61.399 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 4.1629\n",
      "  Batch 200/4191 | Loss: 4.6502\n",
      "  Batch 300/4191 | Loss: 3.7804\n",
      "  Batch 400/4191 | Loss: 3.9451\n",
      "  Batch 500/4191 | Loss: 3.8535\n",
      "  Batch 600/4191 | Loss: 3.7075\n",
      "  Batch 700/4191 | Loss: 3.6077\n",
      "  Batch 800/4191 | Loss: 3.9130\n",
      "  Batch 900/4191 | Loss: 4.0512\n",
      "  Batch 1000/4191 | Loss: 3.3586\n",
      "  Batch 1100/4191 | Loss: 3.8995\n",
      "  Batch 1200/4191 | Loss: 3.5666\n",
      "  Batch 1300/4191 | Loss: 3.4832\n",
      "  Batch 1400/4191 | Loss: 3.8771\n",
      "  Batch 1500/4191 | Loss: 4.0208\n",
      "  Batch 1600/4191 | Loss: 3.9556\n",
      "  Batch 1700/4191 | Loss: 4.0935\n",
      "  Batch 1800/4191 | Loss: 3.6308\n",
      "  Batch 1900/4191 | Loss: 3.5814\n",
      "  Batch 2000/4191 | Loss: 3.7208\n",
      "  Batch 2100/4191 | Loss: 4.0043\n",
      "  Batch 2200/4191 | Loss: 4.1367\n",
      "  Batch 2300/4191 | Loss: 3.6442\n",
      "  Batch 2400/4191 | Loss: 4.2417\n",
      "  Batch 2500/4191 | Loss: 3.7464\n",
      "  Batch 2600/4191 | Loss: 4.2158\n",
      "  Batch 2700/4191 | Loss: 3.6787\n",
      "  Batch 2800/4191 | Loss: 4.0979\n",
      "  Batch 2900/4191 | Loss: 4.1936\n",
      "  Batch 3000/4191 | Loss: 3.8012\n",
      "  Batch 3100/4191 | Loss: 3.4378\n",
      "  Batch 3200/4191 | Loss: 3.5680\n",
      "  Batch 3300/4191 | Loss: 3.7693\n",
      "  Batch 3400/4191 | Loss: 4.3403\n",
      "  Batch 3500/4191 | Loss: 4.1072\n",
      "  Batch 3600/4191 | Loss: 3.8978\n",
      "  Batch 3700/4191 | Loss: 3.9419\n",
      "  Batch 3800/4191 | Loss: 3.6837\n",
      "  Batch 3900/4191 | Loss: 3.7353\n",
      "  Batch 4000/4191 | Loss: 4.1882\n",
      "  Batch 4100/4191 | Loss: 4.3075\n",
      "\n",
      "Epoch: 07 | Time: 4m 28s\n",
      "\tTrain Loss: 3.895 | Train PPL:  49.169\n",
      "\t Val. Loss: 4.065 |  Val. PPL:  58.293 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 4.2727\n",
      "  Batch 200/4191 | Loss: 3.6236\n",
      "  Batch 300/4191 | Loss: 4.1345\n",
      "  Batch 400/4191 | Loss: 3.7628\n",
      "  Batch 500/4191 | Loss: 4.0541\n",
      "  Batch 600/4191 | Loss: 4.0857\n",
      "  Batch 700/4191 | Loss: 3.3124\n",
      "  Batch 800/4191 | Loss: 3.8697\n",
      "  Batch 900/4191 | Loss: 3.5770\n",
      "  Batch 1000/4191 | Loss: 3.8937\n",
      "  Batch 1100/4191 | Loss: 4.1424\n",
      "  Batch 1200/4191 | Loss: 3.6090\n",
      "  Batch 1300/4191 | Loss: 3.6220\n",
      "  Batch 1400/4191 | Loss: 4.2868\n",
      "  Batch 1500/4191 | Loss: 3.2858\n",
      "  Batch 1600/4191 | Loss: 3.5243\n",
      "  Batch 1700/4191 | Loss: 4.1337\n",
      "  Batch 1800/4191 | Loss: 3.5577\n",
      "  Batch 1900/4191 | Loss: 3.5350\n",
      "  Batch 2000/4191 | Loss: 3.9841\n",
      "  Batch 2100/4191 | Loss: 3.9030\n",
      "  Batch 2200/4191 | Loss: 4.0457\n",
      "  Batch 2300/4191 | Loss: 3.9309\n",
      "  Batch 2400/4191 | Loss: 3.8048\n",
      "  Batch 2500/4191 | Loss: 4.1591\n",
      "  Batch 2600/4191 | Loss: 3.4841\n",
      "  Batch 2700/4191 | Loss: 3.3432\n",
      "  Batch 2800/4191 | Loss: 4.1660\n",
      "  Batch 2900/4191 | Loss: 4.0622\n",
      "  Batch 3000/4191 | Loss: 4.2368\n",
      "  Batch 3100/4191 | Loss: 3.7662\n",
      "  Batch 3200/4191 | Loss: 3.4924\n",
      "  Batch 3300/4191 | Loss: 3.9921\n",
      "  Batch 3400/4191 | Loss: 3.8790\n",
      "  Batch 3500/4191 | Loss: 3.8570\n",
      "  Batch 3600/4191 | Loss: 3.7924\n",
      "  Batch 3700/4191 | Loss: 3.7448\n",
      "  Batch 3800/4191 | Loss: 4.3947\n",
      "  Batch 3900/4191 | Loss: 4.3576\n",
      "  Batch 4000/4191 | Loss: 3.7628\n",
      "  Batch 4100/4191 | Loss: 3.5192\n",
      "\n",
      "Epoch: 08 | Time: 4m 29s\n",
      "\tTrain Loss: 3.837 | Train PPL:  46.377\n",
      "\t Val. Loss: 4.050 |  Val. PPL:  57.396 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.4779\n",
      "  Batch 200/4191 | Loss: 3.5410\n",
      "  Batch 300/4191 | Loss: 3.6054\n",
      "  Batch 400/4191 | Loss: 3.6408\n",
      "  Batch 500/4191 | Loss: 4.0463\n",
      "  Batch 600/4191 | Loss: 3.6268\n",
      "  Batch 700/4191 | Loss: 3.8327\n",
      "  Batch 800/4191 | Loss: 4.3041\n",
      "  Batch 900/4191 | Loss: 3.5958\n",
      "  Batch 1000/4191 | Loss: 3.5846\n",
      "  Batch 1100/4191 | Loss: 4.0102\n",
      "  Batch 1200/4191 | Loss: 3.2905\n",
      "  Batch 1300/4191 | Loss: 3.7141\n",
      "  Batch 1400/4191 | Loss: 4.0486\n",
      "  Batch 1500/4191 | Loss: 3.9269\n",
      "  Batch 1600/4191 | Loss: 3.7946\n",
      "  Batch 1700/4191 | Loss: 4.1226\n",
      "  Batch 1800/4191 | Loss: 3.6637\n",
      "  Batch 1900/4191 | Loss: 3.7687\n",
      "  Batch 2000/4191 | Loss: 3.4494\n",
      "  Batch 2100/4191 | Loss: 3.4297\n",
      "  Batch 2200/4191 | Loss: 3.5925\n",
      "  Batch 2300/4191 | Loss: 3.7987\n",
      "  Batch 2400/4191 | Loss: 3.6424\n",
      "  Batch 2500/4191 | Loss: 4.1660\n",
      "  Batch 2600/4191 | Loss: 3.5546\n",
      "  Batch 2700/4191 | Loss: 3.7728\n",
      "  Batch 2800/4191 | Loss: 4.2385\n",
      "  Batch 2900/4191 | Loss: 3.5908\n",
      "  Batch 3000/4191 | Loss: 3.9658\n",
      "  Batch 3100/4191 | Loss: 3.8530\n",
      "  Batch 3200/4191 | Loss: 3.8297\n",
      "  Batch 3300/4191 | Loss: 3.8494\n",
      "  Batch 3400/4191 | Loss: 3.5288\n",
      "  Batch 3500/4191 | Loss: 3.9876\n",
      "  Batch 3600/4191 | Loss: 3.4030\n",
      "  Batch 3700/4191 | Loss: 3.3101\n",
      "  Batch 3800/4191 | Loss: 4.0903\n",
      "  Batch 3900/4191 | Loss: 3.9018\n",
      "  Batch 4000/4191 | Loss: 4.0021\n",
      "  Batch 4100/4191 | Loss: 3.7906\n",
      "\n",
      "Epoch: 09 | Time: 4m 35s\n",
      "\tTrain Loss: 3.784 | Train PPL:  43.985\n",
      "\t Val. Loss: 4.036 |  Val. PPL:  56.617 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.8783\n",
      "  Batch 200/4191 | Loss: 3.5226\n",
      "  Batch 300/4191 | Loss: 3.6374\n",
      "  Batch 400/4191 | Loss: 3.9055\n",
      "  Batch 500/4191 | Loss: 3.5450\n",
      "  Batch 600/4191 | Loss: 3.4850\n",
      "  Batch 700/4191 | Loss: 4.0120\n",
      "  Batch 800/4191 | Loss: 3.6065\n",
      "  Batch 900/4191 | Loss: 3.6916\n",
      "  Batch 1000/4191 | Loss: 4.0026\n",
      "  Batch 1100/4191 | Loss: 3.5911\n",
      "  Batch 1200/4191 | Loss: 4.2701\n",
      "  Batch 1300/4191 | Loss: 3.8824\n",
      "  Batch 1400/4191 | Loss: 3.6386\n",
      "  Batch 1500/4191 | Loss: 3.9110\n",
      "  Batch 1600/4191 | Loss: 3.3783\n",
      "  Batch 1700/4191 | Loss: 3.7844\n",
      "  Batch 1800/4191 | Loss: 3.6719\n",
      "  Batch 1900/4191 | Loss: 3.2702\n",
      "  Batch 2000/4191 | Loss: 3.9140\n",
      "  Batch 2100/4191 | Loss: 4.0605\n",
      "  Batch 2200/4191 | Loss: 3.9779\n",
      "  Batch 2300/4191 | Loss: 3.6691\n",
      "  Batch 2400/4191 | Loss: 3.5383\n",
      "  Batch 2500/4191 | Loss: 4.0038\n",
      "  Batch 2600/4191 | Loss: 3.6663\n",
      "  Batch 2700/4191 | Loss: 3.9265\n",
      "  Batch 2800/4191 | Loss: 3.7673\n",
      "  Batch 2900/4191 | Loss: 3.6385\n",
      "  Batch 3000/4191 | Loss: 3.8409\n",
      "  Batch 3100/4191 | Loss: 3.7427\n",
      "  Batch 3200/4191 | Loss: 3.6630\n",
      "  Batch 3300/4191 | Loss: 3.7967\n",
      "  Batch 3400/4191 | Loss: 3.5546\n",
      "  Batch 3500/4191 | Loss: 3.4322\n",
      "  Batch 3600/4191 | Loss: 3.8296\n",
      "  Batch 3700/4191 | Loss: 3.6824\n",
      "  Batch 3800/4191 | Loss: 3.8067\n",
      "  Batch 3900/4191 | Loss: 3.5734\n",
      "  Batch 4000/4191 | Loss: 3.9389\n",
      "  Batch 4100/4191 | Loss: 3.7323\n",
      "\n",
      "Epoch: 10 | Time: 4m 33s\n",
      "\tTrain Loss: 3.737 | Train PPL:  41.979\n",
      "\t Val. Loss: 3.983 |  Val. PPL:  53.675 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.4518\n",
      "  Batch 200/4191 | Loss: 3.1752\n",
      "  Batch 300/4191 | Loss: 2.9393\n",
      "  Batch 400/4191 | Loss: 3.7405\n",
      "  Batch 500/4191 | Loss: 3.9625\n",
      "  Batch 600/4191 | Loss: 3.8076\n",
      "  Batch 700/4191 | Loss: 3.6290\n",
      "  Batch 800/4191 | Loss: 3.6093\n",
      "  Batch 900/4191 | Loss: 3.8372\n",
      "  Batch 1000/4191 | Loss: 3.8790\n",
      "  Batch 1100/4191 | Loss: 3.8989\n",
      "  Batch 1200/4191 | Loss: 3.5533\n",
      "  Batch 1300/4191 | Loss: 3.8095\n",
      "  Batch 1400/4191 | Loss: 3.3878\n",
      "  Batch 1500/4191 | Loss: 3.8071\n",
      "  Batch 1600/4191 | Loss: 3.9477\n",
      "  Batch 1700/4191 | Loss: 3.4117\n",
      "  Batch 1800/4191 | Loss: 3.9569\n",
      "  Batch 1900/4191 | Loss: 3.7878\n",
      "  Batch 2000/4191 | Loss: 3.4672\n",
      "  Batch 2100/4191 | Loss: 3.6008\n",
      "  Batch 2200/4191 | Loss: 3.3170\n",
      "  Batch 2300/4191 | Loss: 3.4660\n",
      "  Batch 2400/4191 | Loss: 4.1892\n",
      "  Batch 2500/4191 | Loss: 3.5048\n",
      "  Batch 2600/4191 | Loss: 3.7548\n",
      "  Batch 2700/4191 | Loss: 3.9059\n",
      "  Batch 2800/4191 | Loss: 3.8704\n",
      "  Batch 2900/4191 | Loss: 3.7353\n",
      "  Batch 3000/4191 | Loss: 3.8103\n",
      "  Batch 3100/4191 | Loss: 4.0431\n",
      "  Batch 3200/4191 | Loss: 3.6998\n",
      "  Batch 3300/4191 | Loss: 4.0601\n",
      "  Batch 3400/4191 | Loss: 4.2806\n",
      "  Batch 3500/4191 | Loss: 3.4413\n",
      "  Batch 3600/4191 | Loss: 3.9861\n",
      "  Batch 3700/4191 | Loss: 3.7238\n",
      "  Batch 3800/4191 | Loss: 3.9510\n",
      "  Batch 3900/4191 | Loss: 3.7289\n",
      "  Batch 4000/4191 | Loss: 3.7664\n",
      "  Batch 4100/4191 | Loss: 3.8047\n",
      "\n",
      "Epoch: 11 | Time: 4m 32s\n",
      "\tTrain Loss: 3.687 | Train PPL:  39.924\n",
      "\t Val. Loss: 3.995 |  Val. PPL:  54.339 | \n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.3275\n",
      "  Batch 200/4191 | Loss: 3.6709\n",
      "  Batch 300/4191 | Loss: 3.4778\n",
      "  Batch 400/4191 | Loss: 4.0105\n",
      "  Batch 500/4191 | Loss: 4.1041\n",
      "  Batch 600/4191 | Loss: 3.6501\n",
      "  Batch 700/4191 | Loss: 3.5333\n",
      "  Batch 800/4191 | Loss: 3.6734\n",
      "  Batch 900/4191 | Loss: 3.2934\n",
      "  Batch 1000/4191 | Loss: 3.4714\n",
      "  Batch 1100/4191 | Loss: 3.7669\n",
      "  Batch 1200/4191 | Loss: 4.1554\n",
      "  Batch 1300/4191 | Loss: 3.5881\n",
      "  Batch 1400/4191 | Loss: 3.7973\n",
      "  Batch 1500/4191 | Loss: 3.6542\n",
      "  Batch 1600/4191 | Loss: 4.0998\n",
      "  Batch 1700/4191 | Loss: 3.7793\n",
      "  Batch 1800/4191 | Loss: 3.3983\n",
      "  Batch 1900/4191 | Loss: 3.4985\n",
      "  Batch 2000/4191 | Loss: 4.0920\n",
      "  Batch 2100/4191 | Loss: 3.2848\n",
      "  Batch 2200/4191 | Loss: 3.4947\n",
      "  Batch 2300/4191 | Loss: 3.6689\n",
      "  Batch 2400/4191 | Loss: 3.8508\n",
      "  Batch 2500/4191 | Loss: 3.6871\n",
      "  Batch 2600/4191 | Loss: 3.6120\n",
      "  Batch 2700/4191 | Loss: 3.3613\n",
      "  Batch 2800/4191 | Loss: 3.6934\n",
      "  Batch 2900/4191 | Loss: 3.3541\n",
      "  Batch 3000/4191 | Loss: 3.7400\n",
      "  Batch 3100/4191 | Loss: 3.7091\n",
      "  Batch 3200/4191 | Loss: 3.5334\n",
      "  Batch 3300/4191 | Loss: 3.9121\n",
      "  Batch 3400/4191 | Loss: 3.0732\n",
      "  Batch 3500/4191 | Loss: 3.7044\n",
      "  Batch 3600/4191 | Loss: 3.5118\n",
      "  Batch 3700/4191 | Loss: 3.5361\n",
      "  Batch 3800/4191 | Loss: 3.6845\n",
      "  Batch 3900/4191 | Loss: 3.7025\n",
      "  Batch 4000/4191 | Loss: 4.0292\n",
      "  Batch 4100/4191 | Loss: 3.4282\n",
      "\n",
      "Epoch: 12 | Time: 4m 28s\n",
      "\tTrain Loss: 3.641 | Train PPL:  38.144\n",
      "\t Val. Loss: 3.954 |  Val. PPL:  52.152 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.3613\n",
      "  Batch 200/4191 | Loss: 3.2881\n",
      "  Batch 300/4191 | Loss: 3.5560\n",
      "  Batch 400/4191 | Loss: 3.5550\n",
      "  Batch 500/4191 | Loss: 3.3724\n",
      "  Batch 600/4191 | Loss: 3.5383\n",
      "  Batch 700/4191 | Loss: 3.4157\n",
      "  Batch 800/4191 | Loss: 3.7792\n",
      "  Batch 900/4191 | Loss: 2.9952\n",
      "  Batch 1000/4191 | Loss: 4.0437\n",
      "  Batch 1100/4191 | Loss: 3.5798\n",
      "  Batch 1200/4191 | Loss: 3.2703\n",
      "  Batch 1300/4191 | Loss: 3.3803\n",
      "  Batch 1400/4191 | Loss: 3.6498\n",
      "  Batch 1500/4191 | Loss: 3.7932\n",
      "  Batch 1600/4191 | Loss: 3.4388\n",
      "  Batch 1700/4191 | Loss: 3.7836\n",
      "  Batch 1800/4191 | Loss: 3.5136\n",
      "  Batch 1900/4191 | Loss: 3.1746\n",
      "  Batch 2000/4191 | Loss: 3.4270\n",
      "  Batch 2100/4191 | Loss: 3.4297\n",
      "  Batch 2200/4191 | Loss: 3.6038\n",
      "  Batch 2300/4191 | Loss: 3.6698\n",
      "  Batch 2400/4191 | Loss: 3.7866\n",
      "  Batch 2500/4191 | Loss: 3.9109\n",
      "  Batch 2600/4191 | Loss: 3.3683\n",
      "  Batch 2700/4191 | Loss: 3.5902\n",
      "  Batch 2800/4191 | Loss: 3.5968\n",
      "  Batch 2900/4191 | Loss: 3.7204\n",
      "  Batch 3000/4191 | Loss: 3.4665\n",
      "  Batch 3100/4191 | Loss: 4.1298\n",
      "  Batch 3200/4191 | Loss: 3.3858\n",
      "  Batch 3300/4191 | Loss: 3.8769\n",
      "  Batch 3400/4191 | Loss: 3.6988\n",
      "  Batch 3500/4191 | Loss: 3.4039\n",
      "  Batch 3600/4191 | Loss: 4.0229\n",
      "  Batch 3700/4191 | Loss: 3.3579\n",
      "  Batch 3800/4191 | Loss: 3.7267\n",
      "  Batch 3900/4191 | Loss: 3.7550\n",
      "  Batch 4000/4191 | Loss: 3.5182\n",
      "  Batch 4100/4191 | Loss: 3.8661\n",
      "\n",
      "Epoch: 13 | Time: 4m 18s\n",
      "\tTrain Loss: 3.597 | Train PPL:  36.499\n",
      "\t Val. Loss: 3.954 |  Val. PPL:  52.127 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.5317\n",
      "  Batch 200/4191 | Loss: 3.2062\n",
      "  Batch 300/4191 | Loss: 3.5597\n",
      "  Batch 400/4191 | Loss: 3.4966\n",
      "  Batch 500/4191 | Loss: 3.5732\n",
      "  Batch 600/4191 | Loss: 3.5831\n",
      "  Batch 700/4191 | Loss: 3.6784\n",
      "  Batch 800/4191 | Loss: 3.0881\n",
      "  Batch 900/4191 | Loss: 3.5071\n",
      "  Batch 1000/4191 | Loss: 3.1549\n",
      "  Batch 1100/4191 | Loss: 3.7529\n",
      "  Batch 1200/4191 | Loss: 3.3936\n",
      "  Batch 1300/4191 | Loss: 3.5346\n",
      "  Batch 1400/4191 | Loss: 3.4989\n",
      "  Batch 1500/4191 | Loss: 3.8825\n",
      "  Batch 1600/4191 | Loss: 3.5511\n",
      "  Batch 1700/4191 | Loss: 3.5670\n",
      "  Batch 1800/4191 | Loss: 3.9498\n",
      "  Batch 1900/4191 | Loss: 3.7735\n",
      "  Batch 2000/4191 | Loss: 3.1999\n",
      "  Batch 2100/4191 | Loss: 3.6253\n",
      "  Batch 2200/4191 | Loss: 3.6045\n",
      "  Batch 2300/4191 | Loss: 3.5491\n",
      "  Batch 2400/4191 | Loss: 3.8202\n",
      "  Batch 2500/4191 | Loss: 3.2661\n",
      "  Batch 2600/4191 | Loss: 4.0623\n",
      "  Batch 2700/4191 | Loss: 3.8570\n",
      "  Batch 2800/4191 | Loss: 3.4027\n",
      "  Batch 2900/4191 | Loss: 3.7329\n",
      "  Batch 3000/4191 | Loss: 3.5100\n",
      "  Batch 3100/4191 | Loss: 3.4745\n",
      "  Batch 3200/4191 | Loss: 3.5200\n",
      "  Batch 3300/4191 | Loss: 3.3034\n",
      "  Batch 3400/4191 | Loss: 3.7482\n",
      "  Batch 3500/4191 | Loss: 3.5584\n",
      "  Batch 3600/4191 | Loss: 3.4402\n",
      "  Batch 3700/4191 | Loss: 3.3517\n",
      "  Batch 3800/4191 | Loss: 3.7775\n",
      "  Batch 3900/4191 | Loss: 3.9797\n",
      "  Batch 4000/4191 | Loss: 4.0239\n",
      "  Batch 4100/4191 | Loss: 3.2989\n",
      "\n",
      "Epoch: 14 | Time: 4m 27s\n",
      "\tTrain Loss: 3.554 | Train PPL:  34.944\n",
      "\t Val. Loss: 3.901 |  Val. PPL:  49.465 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.0355\n",
      "  Batch 200/4191 | Loss: 3.5297\n",
      "  Batch 300/4191 | Loss: 3.6931\n",
      "  Batch 400/4191 | Loss: 3.3312\n",
      "  Batch 500/4191 | Loss: 3.5815\n",
      "  Batch 600/4191 | Loss: 3.4845\n",
      "  Batch 700/4191 | Loss: 3.8128\n",
      "  Batch 800/4191 | Loss: 3.4686\n",
      "  Batch 900/4191 | Loss: 3.7816\n",
      "  Batch 1000/4191 | Loss: 3.2201\n",
      "  Batch 1100/4191 | Loss: 3.3388\n",
      "  Batch 1200/4191 | Loss: 3.7752\n",
      "  Batch 1300/4191 | Loss: 3.5408\n",
      "  Batch 1400/4191 | Loss: 3.3931\n",
      "  Batch 1500/4191 | Loss: 3.4003\n",
      "  Batch 1600/4191 | Loss: 3.3817\n",
      "  Batch 1700/4191 | Loss: 3.3888\n",
      "  Batch 1800/4191 | Loss: 3.6147\n",
      "  Batch 1900/4191 | Loss: 3.3210\n",
      "  Batch 2000/4191 | Loss: 3.7900\n",
      "  Batch 2100/4191 | Loss: 3.3547\n",
      "  Batch 2200/4191 | Loss: 3.7948\n",
      "  Batch 2300/4191 | Loss: 4.1955\n",
      "  Batch 2400/4191 | Loss: 3.2865\n",
      "  Batch 2500/4191 | Loss: 3.6561\n",
      "  Batch 2600/4191 | Loss: 3.7121\n",
      "  Batch 2700/4191 | Loss: 3.2008\n",
      "  Batch 2800/4191 | Loss: 3.5129\n",
      "  Batch 2900/4191 | Loss: 4.0516\n",
      "  Batch 3000/4191 | Loss: 3.5898\n",
      "  Batch 3100/4191 | Loss: 3.3060\n",
      "  Batch 3200/4191 | Loss: 3.3152\n",
      "  Batch 3300/4191 | Loss: 3.6359\n",
      "  Batch 3400/4191 | Loss: 3.6945\n",
      "  Batch 3500/4191 | Loss: 3.8597\n",
      "  Batch 3600/4191 | Loss: 3.9255\n",
      "  Batch 3700/4191 | Loss: 3.4148\n",
      "  Batch 3800/4191 | Loss: 3.6202\n",
      "  Batch 3900/4191 | Loss: 3.0093\n",
      "  Batch 4000/4191 | Loss: 3.4874\n",
      "  Batch 4100/4191 | Loss: 3.4114\n",
      "\n",
      "Epoch: 15 | Time: 4m 29s\n",
      "\tTrain Loss: 3.509 | Train PPL:  33.415\n",
      "\t Val. Loss: 3.911 |  Val. PPL:  49.934 | \n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.7746\n",
      "  Batch 200/4191 | Loss: 3.3378\n",
      "  Batch 300/4191 | Loss: 3.4815\n",
      "  Batch 400/4191 | Loss: 3.2387\n",
      "  Batch 500/4191 | Loss: 3.5794\n",
      "  Batch 600/4191 | Loss: 3.1247\n",
      "  Batch 700/4191 | Loss: 3.5403\n",
      "  Batch 800/4191 | Loss: 3.4826\n",
      "  Batch 900/4191 | Loss: 2.9921\n",
      "  Batch 1000/4191 | Loss: 3.3692\n",
      "  Batch 1100/4191 | Loss: 3.7154\n",
      "  Batch 1200/4191 | Loss: 3.6210\n",
      "  Batch 1300/4191 | Loss: 3.6146\n",
      "  Batch 1400/4191 | Loss: 3.6286\n",
      "  Batch 1500/4191 | Loss: 3.5074\n",
      "  Batch 1600/4191 | Loss: 3.7340\n",
      "  Batch 1700/4191 | Loss: 3.3766\n",
      "  Batch 1800/4191 | Loss: 3.5845\n",
      "  Batch 1900/4191 | Loss: 3.1250\n",
      "  Batch 2000/4191 | Loss: 3.3163\n",
      "  Batch 2100/4191 | Loss: 3.4462\n",
      "  Batch 2200/4191 | Loss: 3.4429\n",
      "  Batch 2300/4191 | Loss: 3.6440\n",
      "  Batch 2400/4191 | Loss: 3.3009\n",
      "  Batch 2500/4191 | Loss: 3.7307\n",
      "  Batch 2600/4191 | Loss: 3.5232\n",
      "  Batch 2700/4191 | Loss: 3.7781\n",
      "  Batch 2800/4191 | Loss: 3.4374\n",
      "  Batch 2900/4191 | Loss: 3.8847\n",
      "  Batch 3000/4191 | Loss: 3.4878\n",
      "  Batch 3100/4191 | Loss: 3.4609\n",
      "  Batch 3200/4191 | Loss: 3.1893\n",
      "  Batch 3300/4191 | Loss: 3.7209\n",
      "  Batch 3400/4191 | Loss: 3.4271\n",
      "  Batch 3500/4191 | Loss: 3.6726\n",
      "  Batch 3600/4191 | Loss: 3.4432\n",
      "  Batch 3700/4191 | Loss: 3.6212\n",
      "  Batch 3800/4191 | Loss: 3.5547\n",
      "  Batch 3900/4191 | Loss: 3.3629\n",
      "  Batch 4000/4191 | Loss: 3.7042\n",
      "  Batch 4100/4191 | Loss: 3.6966\n",
      "\n",
      "Epoch: 16 | Time: 4m 27s\n",
      "\tTrain Loss: 3.474 | Train PPL:  32.266\n",
      "\t Val. Loss: 3.883 |  Val. PPL:  48.586 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.6594\n",
      "  Batch 200/4191 | Loss: 3.5975\n",
      "  Batch 300/4191 | Loss: 3.1862\n",
      "  Batch 400/4191 | Loss: 3.2184\n",
      "  Batch 500/4191 | Loss: 3.7227\n",
      "  Batch 600/4191 | Loss: 3.1122\n",
      "  Batch 700/4191 | Loss: 3.1510\n",
      "  Batch 800/4191 | Loss: 3.5856\n",
      "  Batch 900/4191 | Loss: 3.4121\n",
      "  Batch 1000/4191 | Loss: 3.4847\n",
      "  Batch 1100/4191 | Loss: 3.1761\n",
      "  Batch 1200/4191 | Loss: 3.3701\n",
      "  Batch 1300/4191 | Loss: 3.3126\n",
      "  Batch 1400/4191 | Loss: 3.0252\n",
      "  Batch 1500/4191 | Loss: 3.5793\n",
      "  Batch 1600/4191 | Loss: 3.6922\n",
      "  Batch 1700/4191 | Loss: 3.0091\n",
      "  Batch 1800/4191 | Loss: 3.5568\n",
      "  Batch 1900/4191 | Loss: 3.2824\n",
      "  Batch 2000/4191 | Loss: 3.7257\n",
      "  Batch 2100/4191 | Loss: 3.6925\n",
      "  Batch 2200/4191 | Loss: 2.9251\n",
      "  Batch 2300/4191 | Loss: 2.7830\n",
      "  Batch 2400/4191 | Loss: 3.5520\n",
      "  Batch 2500/4191 | Loss: 2.8640\n",
      "  Batch 2600/4191 | Loss: 3.1970\n",
      "  Batch 2700/4191 | Loss: 3.2893\n",
      "  Batch 2800/4191 | Loss: 2.9927\n",
      "  Batch 2900/4191 | Loss: 3.3213\n",
      "  Batch 3000/4191 | Loss: 3.8635\n",
      "  Batch 3100/4191 | Loss: 3.6451\n",
      "  Batch 3200/4191 | Loss: 3.4206\n",
      "  Batch 3300/4191 | Loss: 3.3958\n",
      "  Batch 3400/4191 | Loss: 3.5722\n",
      "  Batch 3500/4191 | Loss: 3.5646\n",
      "  Batch 3600/4191 | Loss: 3.3093\n",
      "  Batch 3700/4191 | Loss: 3.4504\n",
      "  Batch 3800/4191 | Loss: 3.1628\n",
      "  Batch 3900/4191 | Loss: 3.6298\n",
      "  Batch 4000/4191 | Loss: 3.9432\n",
      "  Batch 4100/4191 | Loss: 2.9980\n",
      "\n",
      "Epoch: 17 | Time: 4m 27s\n",
      "\tTrain Loss: 3.434 | Train PPL:  30.992\n",
      "\t Val. Loss: 3.859 |  Val. PPL:  47.433 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.6668\n",
      "  Batch 200/4191 | Loss: 3.2865\n",
      "  Batch 300/4191 | Loss: 3.4793\n",
      "  Batch 400/4191 | Loss: 3.3853\n",
      "  Batch 500/4191 | Loss: 3.3750\n",
      "  Batch 600/4191 | Loss: 3.3233\n",
      "  Batch 700/4191 | Loss: 3.2382\n",
      "  Batch 800/4191 | Loss: 3.3460\n",
      "  Batch 900/4191 | Loss: 3.2594\n",
      "  Batch 1000/4191 | Loss: 3.6999\n",
      "  Batch 1100/4191 | Loss: 3.4889\n",
      "  Batch 1200/4191 | Loss: 3.2119\n",
      "  Batch 1300/4191 | Loss: 3.1407\n",
      "  Batch 1400/4191 | Loss: 3.3934\n",
      "  Batch 1500/4191 | Loss: 3.3245\n",
      "  Batch 1600/4191 | Loss: 3.2039\n",
      "  Batch 1700/4191 | Loss: 3.4626\n",
      "  Batch 1800/4191 | Loss: 3.3563\n",
      "  Batch 1900/4191 | Loss: 3.8673\n",
      "  Batch 2000/4191 | Loss: 3.6240\n",
      "  Batch 2100/4191 | Loss: 3.5390\n",
      "  Batch 2200/4191 | Loss: 3.5947\n",
      "  Batch 2300/4191 | Loss: 3.3352\n",
      "  Batch 2400/4191 | Loss: 3.5576\n",
      "  Batch 2500/4191 | Loss: 3.0777\n",
      "  Batch 2600/4191 | Loss: 3.3106\n",
      "  Batch 2700/4191 | Loss: 3.6487\n",
      "  Batch 2800/4191 | Loss: 3.9855\n",
      "  Batch 2900/4191 | Loss: 3.6484\n",
      "  Batch 3000/4191 | Loss: 3.5693\n",
      "  Batch 3100/4191 | Loss: 3.6847\n",
      "  Batch 3200/4191 | Loss: 3.5303\n",
      "  Batch 3300/4191 | Loss: 3.2181\n",
      "  Batch 3400/4191 | Loss: 3.4040\n",
      "  Batch 3500/4191 | Loss: 3.7819\n",
      "  Batch 3600/4191 | Loss: 2.9832\n",
      "  Batch 3700/4191 | Loss: 3.2250\n",
      "  Batch 3800/4191 | Loss: 3.6495\n",
      "  Batch 3900/4191 | Loss: 3.6157\n",
      "  Batch 4000/4191 | Loss: 3.1322\n",
      "  Batch 4100/4191 | Loss: 3.4832\n",
      "\n",
      "Epoch: 18 | Time: 4m 20s\n",
      "\tTrain Loss: 3.397 | Train PPL:  29.889\n",
      "\t Val. Loss: 3.860 |  Val. PPL:  47.472 | \n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.1295\n",
      "  Batch 200/4191 | Loss: 3.2334\n",
      "  Batch 300/4191 | Loss: 3.0775\n",
      "  Batch 400/4191 | Loss: 2.7808\n",
      "  Batch 500/4191 | Loss: 3.0601\n",
      "  Batch 600/4191 | Loss: 3.4327\n",
      "  Batch 700/4191 | Loss: 3.2679\n",
      "  Batch 800/4191 | Loss: 2.9812\n",
      "  Batch 900/4191 | Loss: 3.3925\n",
      "  Batch 1000/4191 | Loss: 3.1063\n",
      "  Batch 1100/4191 | Loss: 3.2405\n",
      "  Batch 1200/4191 | Loss: 3.4948\n",
      "  Batch 1300/4191 | Loss: 3.5765\n",
      "  Batch 1400/4191 | Loss: 3.3294\n",
      "  Batch 1500/4191 | Loss: 3.2076\n",
      "  Batch 1600/4191 | Loss: 3.5602\n",
      "  Batch 1700/4191 | Loss: 3.4033\n",
      "  Batch 1800/4191 | Loss: 3.0476\n",
      "  Batch 1900/4191 | Loss: 3.5943\n",
      "  Batch 2000/4191 | Loss: 3.3891\n",
      "  Batch 2100/4191 | Loss: 3.5300\n",
      "  Batch 2200/4191 | Loss: 3.6142\n",
      "  Batch 2300/4191 | Loss: 3.9646\n",
      "  Batch 2400/4191 | Loss: 3.3562\n",
      "  Batch 2500/4191 | Loss: 3.5839\n",
      "  Batch 2600/4191 | Loss: 3.7563\n",
      "  Batch 2700/4191 | Loss: 3.1682\n",
      "  Batch 2800/4191 | Loss: 3.4965\n",
      "  Batch 2900/4191 | Loss: 3.3040\n",
      "  Batch 3000/4191 | Loss: 3.5780\n",
      "  Batch 3100/4191 | Loss: 3.4546\n",
      "  Batch 3200/4191 | Loss: 3.1570\n",
      "  Batch 3300/4191 | Loss: 3.7952\n",
      "  Batch 3400/4191 | Loss: 3.2545\n",
      "  Batch 3500/4191 | Loss: 3.3426\n",
      "  Batch 3600/4191 | Loss: 3.1860\n",
      "  Batch 3700/4191 | Loss: 3.2512\n",
      "  Batch 3800/4191 | Loss: 3.5528\n",
      "  Batch 3900/4191 | Loss: 3.4904\n",
      "  Batch 4000/4191 | Loss: 3.5500\n",
      "  Batch 4100/4191 | Loss: 3.4065\n",
      "\n",
      "Epoch: 19 | Time: 4m 25s\n",
      "\tTrain Loss: 3.362 | Train PPL:  28.846\n",
      "\t Val. Loss: 3.837 |  Val. PPL:  46.384 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.0989\n",
      "  Batch 200/4191 | Loss: 2.9334\n",
      "  Batch 300/4191 | Loss: 3.2934\n",
      "  Batch 400/4191 | Loss: 3.4212\n",
      "  Batch 500/4191 | Loss: 3.0432\n",
      "  Batch 600/4191 | Loss: 3.2176\n",
      "  Batch 700/4191 | Loss: 3.3704\n",
      "  Batch 800/4191 | Loss: 3.5407\n",
      "  Batch 900/4191 | Loss: 2.8677\n",
      "  Batch 1000/4191 | Loss: 3.4369\n",
      "  Batch 1100/4191 | Loss: 3.3412\n",
      "  Batch 1200/4191 | Loss: 3.6367\n",
      "  Batch 1300/4191 | Loss: 3.4308\n",
      "  Batch 1400/4191 | Loss: 3.6467\n",
      "  Batch 1500/4191 | Loss: 3.5690\n",
      "  Batch 1600/4191 | Loss: 2.9393\n",
      "  Batch 1700/4191 | Loss: 3.1663\n",
      "  Batch 1800/4191 | Loss: 3.3417\n",
      "  Batch 1900/4191 | Loss: 3.2168\n",
      "  Batch 2000/4191 | Loss: 3.3697\n",
      "  Batch 2100/4191 | Loss: 3.7866\n",
      "  Batch 2200/4191 | Loss: 3.4064\n",
      "  Batch 2300/4191 | Loss: 3.0208\n",
      "  Batch 2400/4191 | Loss: 3.6228\n",
      "  Batch 2500/4191 | Loss: 3.5933\n",
      "  Batch 2600/4191 | Loss: 3.0095\n",
      "  Batch 2700/4191 | Loss: 3.2020\n",
      "  Batch 2800/4191 | Loss: 3.4758\n",
      "  Batch 2900/4191 | Loss: 3.2958\n",
      "  Batch 3000/4191 | Loss: 3.5159\n",
      "  Batch 3100/4191 | Loss: 3.5590\n",
      "  Batch 3200/4191 | Loss: 3.6310\n",
      "  Batch 3300/4191 | Loss: 3.0498\n",
      "  Batch 3400/4191 | Loss: 3.5917\n",
      "  Batch 3500/4191 | Loss: 3.3230\n",
      "  Batch 3600/4191 | Loss: 4.0705\n",
      "  Batch 3700/4191 | Loss: 3.4243\n",
      "  Batch 3800/4191 | Loss: 3.1758\n",
      "  Batch 3900/4191 | Loss: 3.1612\n",
      "  Batch 4000/4191 | Loss: 3.6111\n",
      "  Batch 4100/4191 | Loss: 3.5870\n",
      "\n",
      "Epoch: 20 | Time: 4m 29s\n",
      "\tTrain Loss: 3.323 | Train PPL:  27.752\n",
      "\t Val. Loss: 3.823 |  Val. PPL:  45.722 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.8121\n",
      "  Batch 200/4191 | Loss: 3.5131\n",
      "  Batch 300/4191 | Loss: 3.1866\n",
      "  Batch 400/4191 | Loss: 3.7281\n",
      "  Batch 500/4191 | Loss: 2.9443\n",
      "  Batch 600/4191 | Loss: 3.2566\n",
      "  Batch 700/4191 | Loss: 3.2383\n",
      "  Batch 800/4191 | Loss: 3.0352\n",
      "  Batch 900/4191 | Loss: 2.9290\n",
      "  Batch 1000/4191 | Loss: 3.2331\n",
      "  Batch 1100/4191 | Loss: 3.3282\n",
      "  Batch 1200/4191 | Loss: 3.5708\n",
      "  Batch 1300/4191 | Loss: 3.3129\n",
      "  Batch 1400/4191 | Loss: 3.3645\n",
      "  Batch 1500/4191 | Loss: 3.2279\n",
      "  Batch 1600/4191 | Loss: 3.4228\n",
      "  Batch 1700/4191 | Loss: 3.2620\n",
      "  Batch 1800/4191 | Loss: 3.4612\n",
      "  Batch 1900/4191 | Loss: 3.0953\n",
      "  Batch 2000/4191 | Loss: 2.8124\n",
      "  Batch 2100/4191 | Loss: 3.4554\n",
      "  Batch 2200/4191 | Loss: 3.3266\n",
      "  Batch 2300/4191 | Loss: 3.1056\n",
      "  Batch 2400/4191 | Loss: 3.2055\n",
      "  Batch 2500/4191 | Loss: 3.1706\n",
      "  Batch 2600/4191 | Loss: 3.7084\n",
      "  Batch 2700/4191 | Loss: 3.7354\n",
      "  Batch 2800/4191 | Loss: 3.3281\n",
      "  Batch 2900/4191 | Loss: 3.2704\n",
      "  Batch 3000/4191 | Loss: 3.0433\n",
      "  Batch 3100/4191 | Loss: 3.4265\n",
      "  Batch 3200/4191 | Loss: 3.6090\n",
      "  Batch 3300/4191 | Loss: 2.8598\n",
      "  Batch 3400/4191 | Loss: 3.1344\n",
      "  Batch 3500/4191 | Loss: 3.3390\n",
      "  Batch 3600/4191 | Loss: 3.4107\n",
      "  Batch 3700/4191 | Loss: 3.1027\n",
      "  Batch 3800/4191 | Loss: 3.0946\n",
      "  Batch 3900/4191 | Loss: 3.6441\n",
      "  Batch 4000/4191 | Loss: 3.1435\n",
      "  Batch 4100/4191 | Loss: 3.4124\n",
      "\n",
      "Epoch: 21 | Time: 4m 18s\n",
      "\tTrain Loss: 3.290 | Train PPL:  26.836\n",
      "\t Val. Loss: 3.814 |  Val. PPL:  45.320 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.5988\n",
      "  Batch 200/4191 | Loss: 3.0854\n",
      "  Batch 300/4191 | Loss: 2.9234\n",
      "  Batch 400/4191 | Loss: 3.2530\n",
      "  Batch 500/4191 | Loss: 3.6680\n",
      "  Batch 600/4191 | Loss: 3.0897\n",
      "  Batch 700/4191 | Loss: 2.9391\n",
      "  Batch 800/4191 | Loss: 2.9448\n",
      "  Batch 900/4191 | Loss: 3.3414\n",
      "  Batch 1000/4191 | Loss: 3.3057\n",
      "  Batch 1100/4191 | Loss: 3.2471\n",
      "  Batch 1200/4191 | Loss: 3.1088\n",
      "  Batch 1300/4191 | Loss: 3.2200\n",
      "  Batch 1400/4191 | Loss: 3.4251\n",
      "  Batch 1500/4191 | Loss: 3.2959\n",
      "  Batch 1600/4191 | Loss: 3.3752\n",
      "  Batch 1700/4191 | Loss: 3.2125\n",
      "  Batch 1800/4191 | Loss: 3.0760\n",
      "  Batch 1900/4191 | Loss: 2.9131\n",
      "  Batch 2000/4191 | Loss: 3.6893\n",
      "  Batch 2100/4191 | Loss: 3.4078\n",
      "  Batch 2200/4191 | Loss: 3.0522\n",
      "  Batch 2300/4191 | Loss: 3.3763\n",
      "  Batch 2400/4191 | Loss: 3.0314\n",
      "  Batch 2500/4191 | Loss: 3.6424\n",
      "  Batch 2600/4191 | Loss: 3.0044\n",
      "  Batch 2700/4191 | Loss: 3.0853\n",
      "  Batch 2800/4191 | Loss: 3.5739\n",
      "  Batch 2900/4191 | Loss: 3.3145\n",
      "  Batch 3000/4191 | Loss: 3.2136\n",
      "  Batch 3100/4191 | Loss: 3.1265\n",
      "  Batch 3200/4191 | Loss: 3.0982\n",
      "  Batch 3300/4191 | Loss: 3.2678\n",
      "  Batch 3400/4191 | Loss: 3.2563\n",
      "  Batch 3500/4191 | Loss: 3.1154\n",
      "  Batch 3600/4191 | Loss: 3.3754\n",
      "  Batch 3700/4191 | Loss: 3.1350\n",
      "  Batch 3800/4191 | Loss: 3.3382\n",
      "  Batch 3900/4191 | Loss: 3.2956\n",
      "  Batch 4000/4191 | Loss: 3.6353\n",
      "  Batch 4100/4191 | Loss: 3.5894\n",
      "\n",
      "Epoch: 22 | Time: 4m 28s\n",
      "\tTrain Loss: 3.253 | Train PPL:  25.880\n",
      "\t Val. Loss: 3.812 |  Val. PPL:  45.244 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.9403\n",
      "  Batch 200/4191 | Loss: 3.3869\n",
      "  Batch 300/4191 | Loss: 3.1240\n",
      "  Batch 400/4191 | Loss: 3.4112\n",
      "  Batch 500/4191 | Loss: 3.3100\n",
      "  Batch 600/4191 | Loss: 3.5469\n",
      "  Batch 700/4191 | Loss: 3.2545\n",
      "  Batch 800/4191 | Loss: 3.3486\n",
      "  Batch 900/4191 | Loss: 3.1187\n",
      "  Batch 1000/4191 | Loss: 3.4670\n",
      "  Batch 1100/4191 | Loss: 3.0368\n",
      "  Batch 1200/4191 | Loss: 3.2319\n",
      "  Batch 1300/4191 | Loss: 3.2677\n",
      "  Batch 1400/4191 | Loss: 3.0649\n",
      "  Batch 1500/4191 | Loss: 3.2051\n",
      "  Batch 1600/4191 | Loss: 3.1073\n",
      "  Batch 1700/4191 | Loss: 3.4500\n",
      "  Batch 1800/4191 | Loss: 2.8477\n",
      "  Batch 1900/4191 | Loss: 3.1139\n",
      "  Batch 2000/4191 | Loss: 3.1180\n",
      "  Batch 2100/4191 | Loss: 3.3136\n",
      "  Batch 2200/4191 | Loss: 3.0482\n",
      "  Batch 2300/4191 | Loss: 3.1829\n",
      "  Batch 2400/4191 | Loss: 3.0503\n",
      "  Batch 2500/4191 | Loss: 3.0077\n",
      "  Batch 2600/4191 | Loss: 3.2530\n",
      "  Batch 2700/4191 | Loss: 3.4738\n",
      "  Batch 2800/4191 | Loss: 3.1008\n",
      "  Batch 2900/4191 | Loss: 3.2790\n",
      "  Batch 3000/4191 | Loss: 3.2966\n",
      "  Batch 3100/4191 | Loss: 3.4015\n",
      "  Batch 3200/4191 | Loss: 3.3316\n",
      "  Batch 3300/4191 | Loss: 3.1334\n",
      "  Batch 3400/4191 | Loss: 3.1535\n",
      "  Batch 3500/4191 | Loss: 3.0684\n",
      "  Batch 3600/4191 | Loss: 3.1800\n",
      "  Batch 3700/4191 | Loss: 3.2054\n",
      "  Batch 3800/4191 | Loss: 3.7909\n",
      "  Batch 3900/4191 | Loss: 3.1263\n",
      "  Batch 4000/4191 | Loss: 3.4895\n",
      "  Batch 4100/4191 | Loss: 3.1189\n",
      "\n",
      "Epoch: 23 | Time: 4m 25s\n",
      "\tTrain Loss: 3.219 | Train PPL:  25.015\n",
      "\t Val. Loss: 3.797 |  Val. PPL:  44.565 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.0622\n",
      "  Batch 200/4191 | Loss: 3.0581\n",
      "  Batch 300/4191 | Loss: 2.6207\n",
      "  Batch 400/4191 | Loss: 3.2303\n",
      "  Batch 500/4191 | Loss: 2.8101\n",
      "  Batch 600/4191 | Loss: 3.0091\n",
      "  Batch 700/4191 | Loss: 3.2184\n",
      "  Batch 800/4191 | Loss: 3.1466\n",
      "  Batch 900/4191 | Loss: 2.9505\n",
      "  Batch 1000/4191 | Loss: 3.3368\n",
      "  Batch 1100/4191 | Loss: 3.4676\n",
      "  Batch 1200/4191 | Loss: 3.3673\n",
      "  Batch 1300/4191 | Loss: 3.4284\n",
      "  Batch 1400/4191 | Loss: 2.9648\n",
      "  Batch 1500/4191 | Loss: 3.0060\n",
      "  Batch 1600/4191 | Loss: 3.4664\n",
      "  Batch 1700/4191 | Loss: 2.8207\n",
      "  Batch 1800/4191 | Loss: 2.8820\n",
      "  Batch 1900/4191 | Loss: 3.4543\n",
      "  Batch 2000/4191 | Loss: 3.0695\n",
      "  Batch 2100/4191 | Loss: 3.2849\n",
      "  Batch 2200/4191 | Loss: 3.3525\n",
      "  Batch 2300/4191 | Loss: 3.0231\n",
      "  Batch 2400/4191 | Loss: 3.6895\n",
      "  Batch 2500/4191 | Loss: 3.2893\n",
      "  Batch 2600/4191 | Loss: 3.6495\n",
      "  Batch 2700/4191 | Loss: 3.4884\n",
      "  Batch 2800/4191 | Loss: 3.4716\n",
      "  Batch 2900/4191 | Loss: 3.2101\n",
      "  Batch 3000/4191 | Loss: 3.0352\n",
      "  Batch 3100/4191 | Loss: 2.7547\n",
      "  Batch 3200/4191 | Loss: 3.2418\n",
      "  Batch 3300/4191 | Loss: 2.6686\n",
      "  Batch 3400/4191 | Loss: 3.1861\n",
      "  Batch 3500/4191 | Loss: 3.1018\n",
      "  Batch 3600/4191 | Loss: 3.2541\n",
      "  Batch 3700/4191 | Loss: 3.1871\n",
      "  Batch 3800/4191 | Loss: 3.2908\n",
      "  Batch 3900/4191 | Loss: 3.4134\n",
      "  Batch 4000/4191 | Loss: 3.3403\n",
      "  Batch 4100/4191 | Loss: 2.9998\n",
      "\n",
      "Epoch: 24 | Time: 4m 27s\n",
      "\tTrain Loss: 3.190 | Train PPL:  24.285\n",
      "\t Val. Loss: 3.783 |  Val. PPL:  43.965 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.9971\n",
      "  Batch 200/4191 | Loss: 3.3631\n",
      "  Batch 300/4191 | Loss: 3.1090\n",
      "  Batch 400/4191 | Loss: 2.8858\n",
      "  Batch 500/4191 | Loss: 3.4206\n",
      "  Batch 600/4191 | Loss: 2.9339\n",
      "  Batch 700/4191 | Loss: 3.0264\n",
      "  Batch 800/4191 | Loss: 2.7313\n",
      "  Batch 900/4191 | Loss: 3.2032\n",
      "  Batch 1000/4191 | Loss: 3.3219\n",
      "  Batch 1100/4191 | Loss: 3.1754\n",
      "  Batch 1200/4191 | Loss: 3.1993\n",
      "  Batch 1300/4191 | Loss: 3.4523\n",
      "  Batch 1400/4191 | Loss: 3.2328\n",
      "  Batch 1500/4191 | Loss: 3.2468\n",
      "  Batch 1600/4191 | Loss: 3.0164\n",
      "  Batch 1700/4191 | Loss: 3.1548\n",
      "  Batch 1800/4191 | Loss: 3.1009\n",
      "  Batch 1900/4191 | Loss: 3.1635\n",
      "  Batch 2000/4191 | Loss: 3.0266\n",
      "  Batch 2100/4191 | Loss: 3.1187\n",
      "  Batch 2200/4191 | Loss: 3.0880\n",
      "  Batch 2300/4191 | Loss: 3.1370\n",
      "  Batch 2400/4191 | Loss: 3.1149\n",
      "  Batch 2500/4191 | Loss: 3.4771\n",
      "  Batch 2600/4191 | Loss: 3.3504\n",
      "  Batch 2700/4191 | Loss: 3.4813\n",
      "  Batch 2800/4191 | Loss: 2.9584\n",
      "  Batch 2900/4191 | Loss: 3.3004\n",
      "  Batch 3000/4191 | Loss: 3.1750\n",
      "  Batch 3100/4191 | Loss: 3.0925\n",
      "  Batch 3200/4191 | Loss: 3.1908\n",
      "  Batch 3300/4191 | Loss: 2.8794\n",
      "  Batch 3400/4191 | Loss: 3.0605\n",
      "  Batch 3500/4191 | Loss: 3.2051\n",
      "  Batch 3600/4191 | Loss: 3.1169\n",
      "  Batch 3700/4191 | Loss: 3.4876\n",
      "  Batch 3800/4191 | Loss: 3.2707\n",
      "  Batch 3900/4191 | Loss: 3.3370\n",
      "  Batch 4000/4191 | Loss: 3.0371\n",
      "  Batch 4100/4191 | Loss: 3.0634\n",
      "\n",
      "Epoch: 25 | Time: 4m 22s\n",
      "\tTrain Loss: 3.159 | Train PPL:  23.538\n",
      "\t Val. Loss: 3.783 |  Val. PPL:  43.937 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.1746\n",
      "  Batch 200/4191 | Loss: 3.2203\n",
      "  Batch 300/4191 | Loss: 2.6325\n",
      "  Batch 400/4191 | Loss: 2.9552\n",
      "  Batch 500/4191 | Loss: 3.3095\n",
      "  Batch 600/4191 | Loss: 3.0529\n",
      "  Batch 700/4191 | Loss: 3.3110\n",
      "  Batch 800/4191 | Loss: 2.9862\n",
      "  Batch 900/4191 | Loss: 3.2301\n",
      "  Batch 1000/4191 | Loss: 3.2426\n",
      "  Batch 1100/4191 | Loss: 2.9280\n",
      "  Batch 1200/4191 | Loss: 3.2349\n",
      "  Batch 1300/4191 | Loss: 2.9381\n",
      "  Batch 1400/4191 | Loss: 3.0946\n",
      "  Batch 1500/4191 | Loss: 3.2273\n",
      "  Batch 1600/4191 | Loss: 3.0103\n",
      "  Batch 1700/4191 | Loss: 3.3919\n",
      "  Batch 1800/4191 | Loss: 3.2833\n",
      "  Batch 1900/4191 | Loss: 3.4362\n",
      "  Batch 2000/4191 | Loss: 3.1220\n",
      "  Batch 2100/4191 | Loss: 3.0762\n",
      "  Batch 2200/4191 | Loss: 3.1592\n",
      "  Batch 2300/4191 | Loss: 3.0488\n",
      "  Batch 2400/4191 | Loss: 2.8554\n",
      "  Batch 2500/4191 | Loss: 3.3355\n",
      "  Batch 2600/4191 | Loss: 3.4306\n",
      "  Batch 2700/4191 | Loss: 2.8167\n",
      "  Batch 2800/4191 | Loss: 2.7073\n",
      "  Batch 2900/4191 | Loss: 3.2456\n",
      "  Batch 3000/4191 | Loss: 3.5015\n",
      "  Batch 3100/4191 | Loss: 3.1909\n",
      "  Batch 3200/4191 | Loss: 3.1478\n",
      "  Batch 3300/4191 | Loss: 3.1061\n",
      "  Batch 3400/4191 | Loss: 3.3770\n",
      "  Batch 3500/4191 | Loss: 3.2301\n",
      "  Batch 3600/4191 | Loss: 3.0811\n",
      "  Batch 3700/4191 | Loss: 3.3831\n",
      "  Batch 3800/4191 | Loss: 3.0184\n",
      "  Batch 3900/4191 | Loss: 3.3833\n",
      "  Batch 4000/4191 | Loss: 3.4795\n",
      "  Batch 4100/4191 | Loss: 3.2228\n",
      "\n",
      "Epoch: 26 | Time: 4m 25s\n",
      "\tTrain Loss: 3.127 | Train PPL:  22.808\n",
      "\t Val. Loss: 3.783 |  Val. PPL:  43.929 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.8934\n",
      "  Batch 200/4191 | Loss: 3.2921\n",
      "  Batch 300/4191 | Loss: 3.2118\n",
      "  Batch 400/4191 | Loss: 3.2393\n",
      "  Batch 500/4191 | Loss: 3.3301\n",
      "  Batch 600/4191 | Loss: 3.1699\n",
      "  Batch 700/4191 | Loss: 3.0484\n",
      "  Batch 800/4191 | Loss: 3.3297\n",
      "  Batch 900/4191 | Loss: 2.8276\n",
      "  Batch 1000/4191 | Loss: 2.9746\n",
      "  Batch 1100/4191 | Loss: 2.8181\n",
      "  Batch 1200/4191 | Loss: 2.8871\n",
      "  Batch 1300/4191 | Loss: 3.0316\n",
      "  Batch 1400/4191 | Loss: 3.3368\n",
      "  Batch 1500/4191 | Loss: 3.2149\n",
      "  Batch 1600/4191 | Loss: 3.1694\n",
      "  Batch 1700/4191 | Loss: 2.8804\n",
      "  Batch 1800/4191 | Loss: 2.8433\n",
      "  Batch 1900/4191 | Loss: 3.2140\n",
      "  Batch 2000/4191 | Loss: 3.4139\n",
      "  Batch 2100/4191 | Loss: 3.2089\n",
      "  Batch 2200/4191 | Loss: 2.9866\n",
      "  Batch 2300/4191 | Loss: 3.0126\n",
      "  Batch 2400/4191 | Loss: 2.9845\n",
      "  Batch 2500/4191 | Loss: 3.1895\n",
      "  Batch 2600/4191 | Loss: 2.8567\n",
      "  Batch 2700/4191 | Loss: 3.7269\n",
      "  Batch 2800/4191 | Loss: 2.9287\n",
      "  Batch 2900/4191 | Loss: 3.2386\n",
      "  Batch 3000/4191 | Loss: 3.0556\n",
      "  Batch 3100/4191 | Loss: 3.4756\n",
      "  Batch 3200/4191 | Loss: 3.1830\n",
      "  Batch 3300/4191 | Loss: 3.4722\n",
      "  Batch 3400/4191 | Loss: 2.9788\n",
      "  Batch 3500/4191 | Loss: 3.0956\n",
      "  Batch 3600/4191 | Loss: 3.4408\n",
      "  Batch 3700/4191 | Loss: 3.2881\n",
      "  Batch 3800/4191 | Loss: 2.8024\n",
      "  Batch 3900/4191 | Loss: 3.2828\n",
      "  Batch 4000/4191 | Loss: 2.9655\n",
      "  Batch 4100/4191 | Loss: 3.4523\n",
      "\n",
      "Epoch: 27 | Time: 4m 23s\n",
      "\tTrain Loss: 3.100 | Train PPL:  22.200\n",
      "\t Val. Loss: 3.754 |  Val. PPL:  42.706 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.7490\n",
      "  Batch 200/4191 | Loss: 2.7774\n",
      "  Batch 300/4191 | Loss: 3.1888\n",
      "  Batch 400/4191 | Loss: 2.7788\n",
      "  Batch 500/4191 | Loss: 3.0725\n",
      "  Batch 600/4191 | Loss: 2.9285\n",
      "  Batch 700/4191 | Loss: 2.6146\n",
      "  Batch 800/4191 | Loss: 2.8491\n",
      "  Batch 900/4191 | Loss: 2.9200\n",
      "  Batch 1000/4191 | Loss: 3.1976\n",
      "  Batch 1100/4191 | Loss: 2.9466\n",
      "  Batch 1200/4191 | Loss: 2.8680\n",
      "  Batch 1300/4191 | Loss: 3.0460\n",
      "  Batch 1400/4191 | Loss: 3.3082\n",
      "  Batch 1500/4191 | Loss: 3.4380\n",
      "  Batch 1600/4191 | Loss: 2.8134\n",
      "  Batch 1700/4191 | Loss: 3.0164\n",
      "  Batch 1800/4191 | Loss: 3.4504\n",
      "  Batch 1900/4191 | Loss: 3.3069\n",
      "  Batch 2000/4191 | Loss: 2.5347\n",
      "  Batch 2100/4191 | Loss: 3.1997\n",
      "  Batch 2200/4191 | Loss: 3.1220\n",
      "  Batch 2300/4191 | Loss: 2.9075\n",
      "  Batch 2400/4191 | Loss: 3.1069\n",
      "  Batch 2500/4191 | Loss: 2.8064\n",
      "  Batch 2600/4191 | Loss: 2.8241\n",
      "  Batch 2700/4191 | Loss: 3.1122\n",
      "  Batch 2800/4191 | Loss: 3.1848\n",
      "  Batch 2900/4191 | Loss: 2.9200\n",
      "  Batch 3000/4191 | Loss: 3.4643\n",
      "  Batch 3100/4191 | Loss: 3.0447\n",
      "  Batch 3200/4191 | Loss: 3.6327\n",
      "  Batch 3300/4191 | Loss: 3.2060\n",
      "  Batch 3400/4191 | Loss: 2.8161\n",
      "  Batch 3500/4191 | Loss: 2.9446\n",
      "  Batch 3600/4191 | Loss: 3.1526\n",
      "  Batch 3700/4191 | Loss: 2.9743\n",
      "  Batch 3800/4191 | Loss: 3.2169\n",
      "  Batch 3900/4191 | Loss: 2.8298\n",
      "  Batch 4000/4191 | Loss: 2.8954\n",
      "  Batch 4100/4191 | Loss: 3.8039\n",
      "\n",
      "Epoch: 28 | Time: 4m 23s\n",
      "\tTrain Loss: 3.071 | Train PPL:  21.553\n",
      "\t Val. Loss: 3.752 |  Val. PPL:  42.594 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 3.1774\n",
      "  Batch 200/4191 | Loss: 3.2220\n",
      "  Batch 300/4191 | Loss: 2.6188\n",
      "  Batch 400/4191 | Loss: 2.6190\n",
      "  Batch 500/4191 | Loss: 3.3597\n",
      "  Batch 600/4191 | Loss: 2.7378\n",
      "  Batch 700/4191 | Loss: 2.8386\n",
      "  Batch 800/4191 | Loss: 2.8804\n",
      "  Batch 900/4191 | Loss: 2.8755\n",
      "  Batch 1000/4191 | Loss: 2.5378\n",
      "  Batch 1100/4191 | Loss: 2.9567\n",
      "  Batch 1200/4191 | Loss: 2.7958\n",
      "  Batch 1300/4191 | Loss: 2.8754\n",
      "  Batch 1400/4191 | Loss: 2.8324\n",
      "  Batch 1500/4191 | Loss: 2.5557\n",
      "  Batch 1600/4191 | Loss: 3.4251\n",
      "  Batch 1700/4191 | Loss: 2.8566\n",
      "  Batch 1800/4191 | Loss: 3.0908\n",
      "  Batch 1900/4191 | Loss: 2.7160\n",
      "  Batch 2000/4191 | Loss: 3.4717\n",
      "  Batch 2100/4191 | Loss: 3.1764\n",
      "  Batch 2200/4191 | Loss: 3.2628\n",
      "  Batch 2300/4191 | Loss: 3.3225\n",
      "  Batch 2400/4191 | Loss: 3.0125\n",
      "  Batch 2500/4191 | Loss: 3.2516\n",
      "  Batch 2600/4191 | Loss: 3.2560\n",
      "  Batch 2700/4191 | Loss: 3.0545\n",
      "  Batch 2800/4191 | Loss: 3.4209\n",
      "  Batch 2900/4191 | Loss: 2.7224\n",
      "  Batch 3000/4191 | Loss: 2.6727\n",
      "  Batch 3100/4191 | Loss: 2.8341\n",
      "  Batch 3200/4191 | Loss: 3.3112\n",
      "  Batch 3300/4191 | Loss: 3.1942\n",
      "  Batch 3400/4191 | Loss: 3.3471\n",
      "  Batch 3500/4191 | Loss: 3.3015\n",
      "  Batch 3600/4191 | Loss: 3.1510\n",
      "  Batch 3700/4191 | Loss: 3.1571\n",
      "  Batch 3800/4191 | Loss: 3.5668\n",
      "  Batch 3900/4191 | Loss: 3.1885\n",
      "  Batch 4000/4191 | Loss: 3.1788\n",
      "  Batch 4100/4191 | Loss: 2.8603\n",
      "\n",
      "Epoch: 29 | Time: 4m 20s\n",
      "\tTrain Loss: 3.040 | Train PPL:  20.907\n",
      "\t Val. Loss: 3.754 |  Val. PPL:  42.673 | \n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.7703\n",
      "  Batch 200/4191 | Loss: 3.2900\n",
      "  Batch 300/4191 | Loss: 2.6828\n",
      "  Batch 400/4191 | Loss: 2.8678\n",
      "  Batch 500/4191 | Loss: 2.5937\n",
      "  Batch 600/4191 | Loss: 2.7162\n",
      "  Batch 700/4191 | Loss: 3.0779\n",
      "  Batch 800/4191 | Loss: 3.1201\n",
      "  Batch 900/4191 | Loss: 2.9760\n",
      "  Batch 1000/4191 | Loss: 3.2195\n",
      "  Batch 1100/4191 | Loss: 2.7270\n",
      "  Batch 1200/4191 | Loss: 3.1142\n",
      "  Batch 1300/4191 | Loss: 3.4162\n",
      "  Batch 1400/4191 | Loss: 3.0919\n",
      "  Batch 1500/4191 | Loss: 2.9332\n",
      "  Batch 1600/4191 | Loss: 2.8066\n",
      "  Batch 1700/4191 | Loss: 3.0397\n",
      "  Batch 1800/4191 | Loss: 2.7733\n",
      "  Batch 1900/4191 | Loss: 2.5999\n",
      "  Batch 2000/4191 | Loss: 3.2501\n",
      "  Batch 2100/4191 | Loss: 3.0681\n",
      "  Batch 2200/4191 | Loss: 2.9885\n",
      "  Batch 2300/4191 | Loss: 3.3128\n",
      "  Batch 2400/4191 | Loss: 3.1166\n",
      "  Batch 2500/4191 | Loss: 3.0332\n",
      "  Batch 2600/4191 | Loss: 2.8519\n",
      "  Batch 2700/4191 | Loss: 3.0768\n",
      "  Batch 2800/4191 | Loss: 3.0752\n",
      "  Batch 2900/4191 | Loss: 2.6494\n",
      "  Batch 3000/4191 | Loss: 2.9390\n",
      "  Batch 3100/4191 | Loss: 3.0793\n",
      "  Batch 3200/4191 | Loss: 3.0408\n",
      "  Batch 3300/4191 | Loss: 3.2677\n",
      "  Batch 3400/4191 | Loss: 3.4043\n",
      "  Batch 3500/4191 | Loss: 3.3052\n",
      "  Batch 3600/4191 | Loss: 3.0740\n",
      "  Batch 3700/4191 | Loss: 2.9488\n",
      "  Batch 3800/4191 | Loss: 2.5273\n",
      "  Batch 3900/4191 | Loss: 2.4010\n",
      "  Batch 4000/4191 | Loss: 2.8481\n",
      "  Batch 4100/4191 | Loss: 3.0575\n",
      "\n",
      "Epoch: 30 | Time: 4m 17s\n",
      "\tTrain Loss: 3.013 | Train PPL:  20.340\n",
      "\t Val. Loss: 3.740 |  Val. PPL:  42.112 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.5937\n",
      "  Batch 200/4191 | Loss: 3.1142\n",
      "  Batch 300/4191 | Loss: 2.6392\n",
      "  Batch 400/4191 | Loss: 2.9363\n",
      "  Batch 500/4191 | Loss: 3.0509\n",
      "  Batch 600/4191 | Loss: 3.1466\n",
      "  Batch 700/4191 | Loss: 3.2499\n",
      "  Batch 800/4191 | Loss: 2.8290\n",
      "  Batch 900/4191 | Loss: 2.6651\n",
      "  Batch 1000/4191 | Loss: 2.8251\n",
      "  Batch 1100/4191 | Loss: 2.8662\n",
      "  Batch 1200/4191 | Loss: 3.1042\n",
      "  Batch 1300/4191 | Loss: 2.8784\n",
      "  Batch 1400/4191 | Loss: 2.8663\n",
      "  Batch 1500/4191 | Loss: 2.8737\n",
      "  Batch 1600/4191 | Loss: 2.8942\n",
      "  Batch 1700/4191 | Loss: 2.8503\n",
      "  Batch 1800/4191 | Loss: 2.7568\n",
      "  Batch 1900/4191 | Loss: 3.0991\n",
      "  Batch 2000/4191 | Loss: 2.9412\n",
      "  Batch 2100/4191 | Loss: 2.9391\n",
      "  Batch 2200/4191 | Loss: 3.1195\n",
      "  Batch 2300/4191 | Loss: 3.0605\n",
      "  Batch 2400/4191 | Loss: 2.7497\n",
      "  Batch 2500/4191 | Loss: 2.9186\n",
      "  Batch 2600/4191 | Loss: 2.7849\n",
      "  Batch 2700/4191 | Loss: 2.8081\n",
      "  Batch 2800/4191 | Loss: 3.1053\n",
      "  Batch 2900/4191 | Loss: 3.4992\n",
      "  Batch 3000/4191 | Loss: 3.0686\n",
      "  Batch 3100/4191 | Loss: 2.7419\n",
      "  Batch 3200/4191 | Loss: 3.5819\n",
      "  Batch 3300/4191 | Loss: 3.6334\n",
      "  Batch 3400/4191 | Loss: 3.0536\n",
      "  Batch 3500/4191 | Loss: 3.0624\n",
      "  Batch 3600/4191 | Loss: 2.9622\n",
      "  Batch 3700/4191 | Loss: 2.8792\n",
      "  Batch 3800/4191 | Loss: 3.1635\n",
      "  Batch 3900/4191 | Loss: 3.1852\n",
      "  Batch 4000/4191 | Loss: 3.0129\n",
      "  Batch 4100/4191 | Loss: 3.0875\n",
      "\n",
      "Epoch: 31 | Time: 4m 17s\n",
      "\tTrain Loss: 2.988 | Train PPL:  19.841\n",
      "\t Val. Loss: 3.738 |  Val. PPL:  42.022 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.8136\n",
      "  Batch 200/4191 | Loss: 2.9536\n",
      "  Batch 300/4191 | Loss: 2.7073\n",
      "  Batch 400/4191 | Loss: 2.9799\n",
      "  Batch 500/4191 | Loss: 3.4049\n",
      "  Batch 600/4191 | Loss: 3.0200\n",
      "  Batch 700/4191 | Loss: 2.9054\n",
      "  Batch 800/4191 | Loss: 2.6912\n",
      "  Batch 900/4191 | Loss: 2.6605\n",
      "  Batch 1000/4191 | Loss: 2.9485\n",
      "  Batch 1100/4191 | Loss: 3.0923\n",
      "  Batch 1200/4191 | Loss: 3.0481\n",
      "  Batch 1300/4191 | Loss: 2.7943\n",
      "  Batch 1400/4191 | Loss: 2.7995\n",
      "  Batch 1500/4191 | Loss: 2.9443\n",
      "  Batch 1600/4191 | Loss: 2.6264\n",
      "  Batch 1700/4191 | Loss: 3.2887\n",
      "  Batch 1800/4191 | Loss: 2.6920\n",
      "  Batch 1900/4191 | Loss: 2.5471\n",
      "  Batch 2000/4191 | Loss: 3.4367\n",
      "  Batch 2100/4191 | Loss: 2.8932\n",
      "  Batch 2200/4191 | Loss: 2.8973\n",
      "  Batch 2300/4191 | Loss: 2.8461\n",
      "  Batch 2400/4191 | Loss: 2.8784\n",
      "  Batch 2500/4191 | Loss: 2.7679\n",
      "  Batch 2600/4191 | Loss: 3.0292\n",
      "  Batch 2700/4191 | Loss: 2.6890\n",
      "  Batch 2800/4191 | Loss: 2.8492\n",
      "  Batch 2900/4191 | Loss: 3.0365\n",
      "  Batch 3000/4191 | Loss: 2.6851\n",
      "  Batch 3100/4191 | Loss: 3.1000\n",
      "  Batch 3200/4191 | Loss: 3.2495\n",
      "  Batch 3300/4191 | Loss: 2.9268\n",
      "  Batch 3400/4191 | Loss: 2.8003\n",
      "  Batch 3500/4191 | Loss: 2.7938\n",
      "  Batch 3600/4191 | Loss: 2.6728\n",
      "  Batch 3700/4191 | Loss: 2.8863\n",
      "  Batch 3800/4191 | Loss: 2.8816\n",
      "  Batch 3900/4191 | Loss: 3.2628\n",
      "  Batch 4000/4191 | Loss: 3.0357\n",
      "  Batch 4100/4191 | Loss: 3.1882\n",
      "\n",
      "Epoch: 32 | Time: 4m 19s\n",
      "\tTrain Loss: 2.958 | Train PPL:  19.251\n",
      "\t Val. Loss: 3.735 |  Val. PPL:  41.872 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.9054\n",
      "  Batch 200/4191 | Loss: 3.2557\n",
      "  Batch 300/4191 | Loss: 2.7197\n",
      "  Batch 400/4191 | Loss: 2.8949\n",
      "  Batch 500/4191 | Loss: 3.0912\n",
      "  Batch 600/4191 | Loss: 2.9296\n",
      "  Batch 700/4191 | Loss: 3.0294\n",
      "  Batch 800/4191 | Loss: 2.7846\n",
      "  Batch 900/4191 | Loss: 2.8261\n",
      "  Batch 1000/4191 | Loss: 2.7714\n",
      "  Batch 1100/4191 | Loss: 2.6496\n",
      "  Batch 1200/4191 | Loss: 2.8186\n",
      "  Batch 1300/4191 | Loss: 2.7745\n",
      "  Batch 1400/4191 | Loss: 2.8620\n",
      "  Batch 1500/4191 | Loss: 3.0185\n",
      "  Batch 1600/4191 | Loss: 3.2210\n",
      "  Batch 1700/4191 | Loss: 2.7648\n",
      "  Batch 1800/4191 | Loss: 3.1920\n",
      "  Batch 1900/4191 | Loss: 2.8715\n",
      "  Batch 2000/4191 | Loss: 2.9172\n",
      "  Batch 2100/4191 | Loss: 3.0472\n",
      "  Batch 2200/4191 | Loss: 2.7083\n",
      "  Batch 2300/4191 | Loss: 3.1263\n",
      "  Batch 2400/4191 | Loss: 2.7005\n",
      "  Batch 2500/4191 | Loss: 2.8064\n",
      "  Batch 2600/4191 | Loss: 2.9757\n",
      "  Batch 2700/4191 | Loss: 3.0870\n",
      "  Batch 2800/4191 | Loss: 2.9207\n",
      "  Batch 2900/4191 | Loss: 3.1966\n",
      "  Batch 3000/4191 | Loss: 2.9024\n",
      "  Batch 3100/4191 | Loss: 2.9493\n",
      "  Batch 3200/4191 | Loss: 2.9363\n",
      "  Batch 3300/4191 | Loss: 2.9751\n",
      "  Batch 3400/4191 | Loss: 3.0722\n",
      "  Batch 3500/4191 | Loss: 3.3315\n",
      "  Batch 3600/4191 | Loss: 2.8320\n",
      "  Batch 3700/4191 | Loss: 2.9075\n",
      "  Batch 3800/4191 | Loss: 3.0538\n",
      "  Batch 3900/4191 | Loss: 2.8470\n",
      "  Batch 4000/4191 | Loss: 2.8862\n",
      "  Batch 4100/4191 | Loss: 3.1558\n",
      "\n",
      "Epoch: 33 | Time: 4m 19s\n",
      "\tTrain Loss: 2.930 | Train PPL:  18.736\n",
      "\t Val. Loss: 3.717 |  Val. PPL:  41.144 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.6898\n",
      "  Batch 200/4191 | Loss: 2.5102\n",
      "  Batch 300/4191 | Loss: 2.9160\n",
      "  Batch 400/4191 | Loss: 3.0561\n",
      "  Batch 500/4191 | Loss: 3.0602\n",
      "  Batch 600/4191 | Loss: 2.8691\n",
      "  Batch 700/4191 | Loss: 3.1306\n",
      "  Batch 800/4191 | Loss: 2.5278\n",
      "  Batch 900/4191 | Loss: 2.8846\n",
      "  Batch 1000/4191 | Loss: 2.8764\n",
      "  Batch 1100/4191 | Loss: 2.7717\n",
      "  Batch 1200/4191 | Loss: 2.8888\n",
      "  Batch 1300/4191 | Loss: 3.0954\n",
      "  Batch 1400/4191 | Loss: 3.0354\n",
      "  Batch 1500/4191 | Loss: 3.0400\n",
      "  Batch 1600/4191 | Loss: 3.1105\n",
      "  Batch 1700/4191 | Loss: 2.6408\n",
      "  Batch 1800/4191 | Loss: 2.8078\n",
      "  Batch 1900/4191 | Loss: 2.7500\n",
      "  Batch 2000/4191 | Loss: 3.2698\n",
      "  Batch 2100/4191 | Loss: 2.7114\n",
      "  Batch 2200/4191 | Loss: 2.8882\n",
      "  Batch 2300/4191 | Loss: 3.1697\n",
      "  Batch 2400/4191 | Loss: 2.7812\n",
      "  Batch 2500/4191 | Loss: 3.2363\n",
      "  Batch 2600/4191 | Loss: 2.8488\n",
      "  Batch 2700/4191 | Loss: 2.6690\n",
      "  Batch 2800/4191 | Loss: 2.7355\n",
      "  Batch 2900/4191 | Loss: 2.7361\n",
      "  Batch 3000/4191 | Loss: 3.1301\n",
      "  Batch 3100/4191 | Loss: 2.6731\n",
      "  Batch 3200/4191 | Loss: 2.9327\n",
      "  Batch 3300/4191 | Loss: 2.2864\n",
      "  Batch 3400/4191 | Loss: 2.7334\n",
      "  Batch 3500/4191 | Loss: 2.6373\n",
      "  Batch 3600/4191 | Loss: 2.9924\n",
      "  Batch 3700/4191 | Loss: 2.8806\n",
      "  Batch 3800/4191 | Loss: 2.6932\n",
      "  Batch 3900/4191 | Loss: 3.1819\n",
      "  Batch 4000/4191 | Loss: 3.0939\n",
      "  Batch 4100/4191 | Loss: 3.2164\n",
      "\n",
      "Epoch: 34 | Time: 4m 18s\n",
      "\tTrain Loss: 2.902 | Train PPL:  18.219\n",
      "\t Val. Loss: 3.708 |  Val. PPL:  40.779 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.5754\n",
      "  Batch 200/4191 | Loss: 2.7737\n",
      "  Batch 300/4191 | Loss: 3.1085\n",
      "  Batch 400/4191 | Loss: 2.8993\n",
      "  Batch 500/4191 | Loss: 2.7217\n",
      "  Batch 600/4191 | Loss: 2.3571\n",
      "  Batch 700/4191 | Loss: 2.9134\n",
      "  Batch 800/4191 | Loss: 2.8326\n",
      "  Batch 900/4191 | Loss: 2.6918\n",
      "  Batch 1000/4191 | Loss: 2.9482\n",
      "  Batch 1100/4191 | Loss: 2.9188\n",
      "  Batch 1200/4191 | Loss: 2.7516\n",
      "  Batch 1300/4191 | Loss: 2.8708\n",
      "  Batch 1400/4191 | Loss: 2.7607\n",
      "  Batch 1500/4191 | Loss: 2.9355\n",
      "  Batch 1600/4191 | Loss: 2.8418\n",
      "  Batch 1700/4191 | Loss: 2.9844\n",
      "  Batch 1800/4191 | Loss: 3.1581\n",
      "  Batch 1900/4191 | Loss: 2.9654\n",
      "  Batch 2000/4191 | Loss: 3.3724\n",
      "  Batch 2100/4191 | Loss: 2.9522\n",
      "  Batch 2200/4191 | Loss: 3.1478\n",
      "  Batch 2300/4191 | Loss: 2.6330\n",
      "  Batch 2400/4191 | Loss: 2.8880\n",
      "  Batch 2500/4191 | Loss: 2.2703\n",
      "  Batch 2600/4191 | Loss: 3.0620\n",
      "  Batch 2700/4191 | Loss: 2.9464\n",
      "  Batch 2800/4191 | Loss: 2.6233\n",
      "  Batch 2900/4191 | Loss: 2.8270\n",
      "  Batch 3000/4191 | Loss: 2.3238\n",
      "  Batch 3100/4191 | Loss: 2.7339\n",
      "  Batch 3200/4191 | Loss: 2.7956\n",
      "  Batch 3300/4191 | Loss: 3.1048\n",
      "  Batch 3400/4191 | Loss: 2.7920\n",
      "  Batch 3500/4191 | Loss: 3.1617\n",
      "  Batch 3600/4191 | Loss: 3.1276\n",
      "  Batch 3700/4191 | Loss: 2.7887\n",
      "  Batch 3800/4191 | Loss: 3.0541\n",
      "  Batch 3900/4191 | Loss: 3.1605\n",
      "  Batch 4000/4191 | Loss: 3.2173\n",
      "  Batch 4100/4191 | Loss: 2.8712\n",
      "\n",
      "Epoch: 35 | Time: 4m 22s\n",
      "\tTrain Loss: 2.879 | Train PPL:  17.804\n",
      "\t Val. Loss: 3.709 |  Val. PPL:  40.796 | \n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.9437\n",
      "  Batch 200/4191 | Loss: 2.9428\n",
      "  Batch 300/4191 | Loss: 2.6136\n",
      "  Batch 400/4191 | Loss: 2.7164\n",
      "  Batch 500/4191 | Loss: 2.5387\n",
      "  Batch 600/4191 | Loss: 2.6256\n",
      "  Batch 700/4191 | Loss: 2.6859\n",
      "  Batch 800/4191 | Loss: 2.7556\n",
      "  Batch 900/4191 | Loss: 2.7774\n",
      "  Batch 1000/4191 | Loss: 3.1510\n",
      "  Batch 1100/4191 | Loss: 2.9367\n",
      "  Batch 1200/4191 | Loss: 2.5056\n",
      "  Batch 1300/4191 | Loss: 2.6120\n",
      "  Batch 1400/4191 | Loss: 2.8081\n",
      "  Batch 1500/4191 | Loss: 2.5316\n",
      "  Batch 1600/4191 | Loss: 2.6643\n",
      "  Batch 1700/4191 | Loss: 3.1233\n",
      "  Batch 1800/4191 | Loss: 3.1800\n",
      "  Batch 1900/4191 | Loss: 2.7474\n",
      "  Batch 2000/4191 | Loss: 2.4471\n",
      "  Batch 2100/4191 | Loss: 2.9503\n",
      "  Batch 2200/4191 | Loss: 2.8687\n",
      "  Batch 2300/4191 | Loss: 2.9191\n",
      "  Batch 2400/4191 | Loss: 3.4034\n",
      "  Batch 2500/4191 | Loss: 2.7133\n",
      "  Batch 2600/4191 | Loss: 3.2326\n",
      "  Batch 2700/4191 | Loss: 2.4300\n",
      "  Batch 2800/4191 | Loss: 2.7642\n",
      "  Batch 2900/4191 | Loss: 2.4876\n",
      "  Batch 3000/4191 | Loss: 3.1097\n",
      "  Batch 3100/4191 | Loss: 2.8917\n",
      "  Batch 3200/4191 | Loss: 2.5669\n",
      "  Batch 3300/4191 | Loss: 2.8741\n",
      "  Batch 3400/4191 | Loss: 2.2276\n",
      "  Batch 3500/4191 | Loss: 2.7171\n",
      "  Batch 3600/4191 | Loss: 2.9100\n",
      "  Batch 3700/4191 | Loss: 2.7311\n",
      "  Batch 3800/4191 | Loss: 2.1958\n",
      "  Batch 3900/4191 | Loss: 2.5516\n",
      "  Batch 4000/4191 | Loss: 3.0597\n",
      "  Batch 4100/4191 | Loss: 3.0098\n",
      "\n",
      "Epoch: 36 | Time: 4m 26s\n",
      "\tTrain Loss: 2.852 | Train PPL:  17.315\n",
      "\t Val. Loss: 3.694 |  Val. PPL:  40.201 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.8805\n",
      "  Batch 200/4191 | Loss: 2.8178\n",
      "  Batch 300/4191 | Loss: 2.8698\n",
      "  Batch 400/4191 | Loss: 2.7301\n",
      "  Batch 500/4191 | Loss: 2.9013\n",
      "  Batch 600/4191 | Loss: 3.1183\n",
      "  Batch 700/4191 | Loss: 2.9933\n",
      "  Batch 800/4191 | Loss: 2.6600\n",
      "  Batch 900/4191 | Loss: 2.6764\n",
      "  Batch 1000/4191 | Loss: 2.7968\n",
      "  Batch 1100/4191 | Loss: 2.9612\n",
      "  Batch 1200/4191 | Loss: 2.5854\n",
      "  Batch 1300/4191 | Loss: 2.8854\n",
      "  Batch 1400/4191 | Loss: 2.9232\n",
      "  Batch 1500/4191 | Loss: 2.8227\n",
      "  Batch 1600/4191 | Loss: 2.5772\n",
      "  Batch 1700/4191 | Loss: 2.9624\n",
      "  Batch 1800/4191 | Loss: 2.7655\n",
      "  Batch 1900/4191 | Loss: 3.2758\n",
      "  Batch 2000/4191 | Loss: 2.5138\n",
      "  Batch 2100/4191 | Loss: 3.0088\n",
      "  Batch 2200/4191 | Loss: 2.7682\n",
      "  Batch 2300/4191 | Loss: 2.9887\n",
      "  Batch 2400/4191 | Loss: 3.0168\n",
      "  Batch 2500/4191 | Loss: 2.5289\n",
      "  Batch 2600/4191 | Loss: 2.5401\n",
      "  Batch 2700/4191 | Loss: 2.9013\n",
      "  Batch 2800/4191 | Loss: 2.8476\n",
      "  Batch 2900/4191 | Loss: 2.9368\n",
      "  Batch 3000/4191 | Loss: 3.2923\n",
      "  Batch 3100/4191 | Loss: 3.0375\n",
      "  Batch 3200/4191 | Loss: 2.8414\n",
      "  Batch 3300/4191 | Loss: 2.4527\n",
      "  Batch 3400/4191 | Loss: 2.9420\n",
      "  Batch 3500/4191 | Loss: 2.9822\n",
      "  Batch 3600/4191 | Loss: 3.2277\n",
      "  Batch 3700/4191 | Loss: 2.8357\n",
      "  Batch 3800/4191 | Loss: 3.1844\n",
      "  Batch 3900/4191 | Loss: 3.1636\n",
      "  Batch 4000/4191 | Loss: 2.8295\n",
      "  Batch 4100/4191 | Loss: 3.2354\n",
      "\n",
      "Epoch: 37 | Time: 4m 24s\n",
      "\tTrain Loss: 2.824 | Train PPL:  16.848\n",
      "\t Val. Loss: 3.679 |  Val. PPL:  39.609 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.7714\n",
      "  Batch 200/4191 | Loss: 2.9416\n",
      "  Batch 300/4191 | Loss: 2.5218\n",
      "  Batch 400/4191 | Loss: 2.8932\n",
      "  Batch 500/4191 | Loss: 2.5749\n",
      "  Batch 600/4191 | Loss: 2.6450\n",
      "  Batch 700/4191 | Loss: 2.8142\n",
      "  Batch 800/4191 | Loss: 2.9924\n",
      "  Batch 900/4191 | Loss: 2.5365\n",
      "  Batch 1000/4191 | Loss: 2.5461\n",
      "  Batch 1100/4191 | Loss: 2.9635\n",
      "  Batch 1200/4191 | Loss: 2.7120\n",
      "  Batch 1300/4191 | Loss: 2.9583\n",
      "  Batch 1400/4191 | Loss: 2.6054\n",
      "  Batch 1500/4191 | Loss: 2.8113\n",
      "  Batch 1600/4191 | Loss: 2.9399\n",
      "  Batch 1700/4191 | Loss: 2.6321\n",
      "  Batch 1800/4191 | Loss: 2.9157\n",
      "  Batch 1900/4191 | Loss: 2.8067\n",
      "  Batch 2000/4191 | Loss: 2.6813\n",
      "  Batch 2100/4191 | Loss: 2.7716\n",
      "  Batch 2200/4191 | Loss: 2.9647\n",
      "  Batch 2300/4191 | Loss: 2.3904\n",
      "  Batch 2400/4191 | Loss: 2.6232\n",
      "  Batch 2500/4191 | Loss: 2.6363\n",
      "  Batch 2600/4191 | Loss: 2.7354\n",
      "  Batch 2700/4191 | Loss: 2.8294\n",
      "  Batch 2800/4191 | Loss: 2.7952\n",
      "  Batch 2900/4191 | Loss: 3.1794\n",
      "  Batch 3000/4191 | Loss: 2.8480\n",
      "  Batch 3100/4191 | Loss: 2.9077\n",
      "  Batch 3200/4191 | Loss: 2.9250\n",
      "  Batch 3300/4191 | Loss: 2.7425\n",
      "  Batch 3400/4191 | Loss: 2.4463\n",
      "  Batch 3500/4191 | Loss: 2.8513\n",
      "  Batch 3600/4191 | Loss: 2.4075\n",
      "  Batch 3700/4191 | Loss: 3.1543\n",
      "  Batch 3800/4191 | Loss: 3.0153\n",
      "  Batch 3900/4191 | Loss: 2.6659\n",
      "  Batch 4000/4191 | Loss: 2.6405\n",
      "  Batch 4100/4191 | Loss: 2.5476\n",
      "\n",
      "Epoch: 38 | Time: 4m 21s\n",
      "\tTrain Loss: 2.794 | Train PPL:  16.349\n",
      "\t Val. Loss: 3.674 |  Val. PPL:  39.418 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.4963\n",
      "  Batch 200/4191 | Loss: 3.2055\n",
      "  Batch 300/4191 | Loss: 2.2707\n",
      "  Batch 400/4191 | Loss: 2.9502\n",
      "  Batch 500/4191 | Loss: 2.7655\n",
      "  Batch 600/4191 | Loss: 2.5434\n",
      "  Batch 700/4191 | Loss: 2.6879\n",
      "  Batch 800/4191 | Loss: 2.9427\n",
      "  Batch 900/4191 | Loss: 2.7031\n",
      "  Batch 1000/4191 | Loss: 3.1403\n",
      "  Batch 1100/4191 | Loss: 3.2680\n",
      "  Batch 1200/4191 | Loss: 2.6219\n",
      "  Batch 1300/4191 | Loss: 2.9692\n",
      "  Batch 1400/4191 | Loss: 2.8329\n",
      "  Batch 1500/4191 | Loss: 2.7811\n",
      "  Batch 1600/4191 | Loss: 2.9325\n",
      "  Batch 1700/4191 | Loss: 2.9681\n",
      "  Batch 1800/4191 | Loss: 2.8701\n",
      "  Batch 1900/4191 | Loss: 2.3564\n",
      "  Batch 2000/4191 | Loss: 2.8860\n",
      "  Batch 2100/4191 | Loss: 2.4914\n",
      "  Batch 2200/4191 | Loss: 2.8381\n",
      "  Batch 2300/4191 | Loss: 2.9687\n",
      "  Batch 2400/4191 | Loss: 2.7343\n",
      "  Batch 2500/4191 | Loss: 2.7307\n",
      "  Batch 2600/4191 | Loss: 2.6891\n",
      "  Batch 2700/4191 | Loss: 2.7799\n",
      "  Batch 2800/4191 | Loss: 3.2339\n",
      "  Batch 2900/4191 | Loss: 2.9851\n",
      "  Batch 3000/4191 | Loss: 2.8182\n",
      "  Batch 3100/4191 | Loss: 2.6814\n",
      "  Batch 3200/4191 | Loss: 2.6320\n",
      "  Batch 3300/4191 | Loss: 3.3220\n",
      "  Batch 3400/4191 | Loss: 2.6965\n",
      "  Batch 3500/4191 | Loss: 2.6510\n",
      "  Batch 3600/4191 | Loss: 2.7112\n",
      "  Batch 3700/4191 | Loss: 2.8336\n",
      "  Batch 3800/4191 | Loss: 3.0171\n",
      "  Batch 3900/4191 | Loss: 3.0662\n",
      "  Batch 4000/4191 | Loss: 2.9939\n",
      "  Batch 4100/4191 | Loss: 2.6859\n",
      "\n",
      "Epoch: 39 | Time: 4m 19s\n",
      "\tTrain Loss: 2.768 | Train PPL:  15.932\n",
      "\t Val. Loss: 3.658 |  Val. PPL:  38.794 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.7922\n",
      "  Batch 200/4191 | Loss: 2.4898\n",
      "  Batch 300/4191 | Loss: 2.6226\n",
      "  Batch 400/4191 | Loss: 2.9643\n",
      "  Batch 500/4191 | Loss: 2.5460\n",
      "  Batch 600/4191 | Loss: 3.1163\n",
      "  Batch 700/4191 | Loss: 2.9784\n",
      "  Batch 800/4191 | Loss: 2.5974\n",
      "  Batch 900/4191 | Loss: 2.5944\n",
      "  Batch 1000/4191 | Loss: 2.6277\n",
      "  Batch 1100/4191 | Loss: 3.0705\n",
      "  Batch 1200/4191 | Loss: 2.6888\n",
      "  Batch 1300/4191 | Loss: 2.9127\n",
      "  Batch 1400/4191 | Loss: 2.6677\n",
      "  Batch 1500/4191 | Loss: 2.9408\n",
      "  Batch 1600/4191 | Loss: 2.5848\n",
      "  Batch 1700/4191 | Loss: 2.8135\n",
      "  Batch 1800/4191 | Loss: 2.4099\n",
      "  Batch 1900/4191 | Loss: 2.7709\n",
      "  Batch 2000/4191 | Loss: 2.7318\n",
      "  Batch 2100/4191 | Loss: 2.8144\n",
      "  Batch 2200/4191 | Loss: 3.1015\n",
      "  Batch 2300/4191 | Loss: 2.8167\n",
      "  Batch 2400/4191 | Loss: 2.5982\n",
      "  Batch 2500/4191 | Loss: 2.6521\n",
      "  Batch 2600/4191 | Loss: 2.2931\n",
      "  Batch 2700/4191 | Loss: 3.1778\n",
      "  Batch 2800/4191 | Loss: 2.3607\n",
      "  Batch 2900/4191 | Loss: 2.8378\n",
      "  Batch 3000/4191 | Loss: 2.8867\n",
      "  Batch 3100/4191 | Loss: 3.1942\n",
      "  Batch 3200/4191 | Loss: 2.9885\n",
      "  Batch 3300/4191 | Loss: 2.8519\n",
      "  Batch 3400/4191 | Loss: 2.9175\n",
      "  Batch 3500/4191 | Loss: 2.8903\n",
      "  Batch 3600/4191 | Loss: 2.6986\n",
      "  Batch 3700/4191 | Loss: 2.8339\n",
      "  Batch 3800/4191 | Loss: 2.7445\n",
      "  Batch 3900/4191 | Loss: 2.3278\n",
      "  Batch 4000/4191 | Loss: 2.9050\n",
      "  Batch 4100/4191 | Loss: 2.9923\n",
      "\n",
      "Epoch: 40 | Time: 4m 20s\n",
      "\tTrain Loss: 2.739 | Train PPL:  15.464\n",
      "\t Val. Loss: 3.646 |  Val. PPL:  38.310 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.8813\n",
      "  Batch 200/4191 | Loss: 2.8796\n",
      "  Batch 300/4191 | Loss: 2.2137\n",
      "  Batch 400/4191 | Loss: 2.6586\n",
      "  Batch 500/4191 | Loss: 2.5739\n",
      "  Batch 600/4191 | Loss: 2.7043\n",
      "  Batch 700/4191 | Loss: 2.9314\n",
      "  Batch 800/4191 | Loss: 2.5229\n",
      "  Batch 900/4191 | Loss: 2.6984\n",
      "  Batch 1000/4191 | Loss: 2.4482\n",
      "  Batch 1100/4191 | Loss: 2.6155\n",
      "  Batch 1200/4191 | Loss: 2.7371\n",
      "  Batch 1300/4191 | Loss: 2.7009\n",
      "  Batch 1400/4191 | Loss: 2.3541\n",
      "  Batch 1500/4191 | Loss: 2.9262\n",
      "  Batch 1600/4191 | Loss: 2.6193\n",
      "  Batch 1700/4191 | Loss: 2.3989\n",
      "  Batch 1800/4191 | Loss: 3.1768\n",
      "  Batch 1900/4191 | Loss: 2.4460\n",
      "  Batch 2000/4191 | Loss: 2.3003\n",
      "  Batch 2100/4191 | Loss: 2.6211\n",
      "  Batch 2200/4191 | Loss: 2.6705\n",
      "  Batch 2300/4191 | Loss: 2.7511\n",
      "  Batch 2400/4191 | Loss: 3.0158\n",
      "  Batch 2500/4191 | Loss: 3.2597\n",
      "  Batch 2600/4191 | Loss: 2.8040\n",
      "  Batch 2700/4191 | Loss: 3.2678\n",
      "  Batch 2800/4191 | Loss: 2.6862\n",
      "  Batch 2900/4191 | Loss: 2.8659\n",
      "  Batch 3000/4191 | Loss: 2.8466\n",
      "  Batch 3100/4191 | Loss: 2.8694\n",
      "  Batch 3200/4191 | Loss: 3.0230\n",
      "  Batch 3300/4191 | Loss: 2.8149\n",
      "  Batch 3400/4191 | Loss: 2.9752\n",
      "  Batch 3500/4191 | Loss: 3.5006\n",
      "  Batch 3600/4191 | Loss: 2.9431\n",
      "  Batch 3700/4191 | Loss: 2.7846\n",
      "  Batch 3800/4191 | Loss: 1.9997\n",
      "  Batch 3900/4191 | Loss: 2.9776\n",
      "  Batch 4000/4191 | Loss: 2.7081\n",
      "  Batch 4100/4191 | Loss: 2.8931\n",
      "\n",
      "Epoch: 41 | Time: 4m 17s\n",
      "\tTrain Loss: 2.710 | Train PPL:  15.029\n",
      "\t Val. Loss: 3.641 |  Val. PPL:  38.145 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.5078\n",
      "  Batch 200/4191 | Loss: 2.5357\n",
      "  Batch 300/4191 | Loss: 2.6912\n",
      "  Batch 400/4191 | Loss: 2.3480\n",
      "  Batch 500/4191 | Loss: 2.4111\n",
      "  Batch 600/4191 | Loss: 2.7103\n",
      "  Batch 700/4191 | Loss: 2.3591\n",
      "  Batch 800/4191 | Loss: 2.9078\n",
      "  Batch 900/4191 | Loss: 2.6957\n",
      "  Batch 1000/4191 | Loss: 2.4774\n",
      "  Batch 1100/4191 | Loss: 2.4792\n",
      "  Batch 1200/4191 | Loss: 2.9503\n",
      "  Batch 1300/4191 | Loss: 2.5152\n",
      "  Batch 1400/4191 | Loss: 2.7382\n",
      "  Batch 1500/4191 | Loss: 3.0933\n",
      "  Batch 1600/4191 | Loss: 2.3485\n",
      "  Batch 1700/4191 | Loss: 2.5248\n",
      "  Batch 1800/4191 | Loss: 2.8325\n",
      "  Batch 1900/4191 | Loss: 2.7237\n",
      "  Batch 2000/4191 | Loss: 2.5565\n",
      "  Batch 2100/4191 | Loss: 2.7104\n",
      "  Batch 2200/4191 | Loss: 2.3521\n",
      "  Batch 2300/4191 | Loss: 2.7839\n",
      "  Batch 2400/4191 | Loss: 2.6245\n",
      "  Batch 2500/4191 | Loss: 2.9147\n",
      "  Batch 2600/4191 | Loss: 2.7030\n",
      "  Batch 2700/4191 | Loss: 2.4282\n",
      "  Batch 2800/4191 | Loss: 2.9763\n",
      "  Batch 2900/4191 | Loss: 2.7770\n",
      "  Batch 3000/4191 | Loss: 2.9202\n",
      "  Batch 3100/4191 | Loss: 2.6685\n",
      "  Batch 3200/4191 | Loss: 2.8371\n",
      "  Batch 3300/4191 | Loss: 3.0965\n",
      "  Batch 3400/4191 | Loss: 2.7740\n",
      "  Batch 3500/4191 | Loss: 2.8460\n",
      "  Batch 3600/4191 | Loss: 2.6871\n",
      "  Batch 3700/4191 | Loss: 2.6246\n",
      "  Batch 3800/4191 | Loss: 2.4749\n",
      "  Batch 3900/4191 | Loss: 2.7560\n",
      "  Batch 4000/4191 | Loss: 2.6709\n",
      "  Batch 4100/4191 | Loss: 2.8944\n",
      "\n",
      "Epoch: 42 | Time: 4m 22s\n",
      "\tTrain Loss: 2.682 | Train PPL:  14.610\n",
      "\t Val. Loss: 3.635 |  Val. PPL:  37.904 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.9055\n",
      "  Batch 200/4191 | Loss: 2.7779\n",
      "  Batch 300/4191 | Loss: 2.7719\n",
      "  Batch 400/4191 | Loss: 2.7226\n",
      "  Batch 500/4191 | Loss: 2.5851\n",
      "  Batch 600/4191 | Loss: 2.7149\n",
      "  Batch 700/4191 | Loss: 2.5540\n",
      "  Batch 800/4191 | Loss: 2.8543\n",
      "  Batch 900/4191 | Loss: 2.7243\n",
      "  Batch 1000/4191 | Loss: 2.4480\n",
      "  Batch 1100/4191 | Loss: 2.4226\n",
      "  Batch 1200/4191 | Loss: 2.8913\n",
      "  Batch 1300/4191 | Loss: 2.4687\n",
      "  Batch 1400/4191 | Loss: 2.2876\n",
      "  Batch 1500/4191 | Loss: 2.3093\n",
      "  Batch 1600/4191 | Loss: 2.3419\n",
      "  Batch 1700/4191 | Loss: 2.9710\n",
      "  Batch 1800/4191 | Loss: 2.6890\n",
      "  Batch 1900/4191 | Loss: 2.7360\n",
      "  Batch 2000/4191 | Loss: 2.5753\n",
      "  Batch 2100/4191 | Loss: 2.4514\n",
      "  Batch 2200/4191 | Loss: 2.5395\n",
      "  Batch 2300/4191 | Loss: 2.7479\n",
      "  Batch 2400/4191 | Loss: 2.6958\n",
      "  Batch 2500/4191 | Loss: 2.5849\n",
      "  Batch 2600/4191 | Loss: 2.6977\n",
      "  Batch 2700/4191 | Loss: 2.5611\n",
      "  Batch 2800/4191 | Loss: 2.8616\n",
      "  Batch 2900/4191 | Loss: 2.6455\n",
      "  Batch 3000/4191 | Loss: 2.7788\n",
      "  Batch 3100/4191 | Loss: 2.3584\n",
      "  Batch 3200/4191 | Loss: 2.6386\n",
      "  Batch 3300/4191 | Loss: 2.4930\n",
      "  Batch 3400/4191 | Loss: 3.2609\n",
      "  Batch 3500/4191 | Loss: 2.3713\n",
      "  Batch 3600/4191 | Loss: 2.8206\n",
      "  Batch 3700/4191 | Loss: 2.5829\n",
      "  Batch 3800/4191 | Loss: 2.8677\n",
      "  Batch 3900/4191 | Loss: 2.7614\n",
      "  Batch 4000/4191 | Loss: 2.9349\n",
      "  Batch 4100/4191 | Loss: 2.9530\n",
      "\n",
      "Epoch: 43 | Time: 4m 17s\n",
      "\tTrain Loss: 2.653 | Train PPL:  14.192\n",
      "\t Val. Loss: 3.628 |  Val. PPL:  37.637 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.3012\n",
      "  Batch 200/4191 | Loss: 2.8719\n",
      "  Batch 300/4191 | Loss: 2.6601\n",
      "  Batch 400/4191 | Loss: 2.8764\n",
      "  Batch 500/4191 | Loss: 2.4718\n",
      "  Batch 600/4191 | Loss: 2.7887\n",
      "  Batch 700/4191 | Loss: 2.9018\n",
      "  Batch 800/4191 | Loss: 2.2445\n",
      "  Batch 900/4191 | Loss: 2.8173\n",
      "  Batch 1000/4191 | Loss: 2.7730\n",
      "  Batch 1100/4191 | Loss: 2.6281\n",
      "  Batch 1200/4191 | Loss: 2.5868\n",
      "  Batch 1300/4191 | Loss: 2.6759\n",
      "  Batch 1400/4191 | Loss: 2.7656\n",
      "  Batch 1500/4191 | Loss: 2.3944\n",
      "  Batch 1600/4191 | Loss: 2.6866\n",
      "  Batch 1700/4191 | Loss: 2.7823\n",
      "  Batch 1800/4191 | Loss: 2.4845\n",
      "  Batch 1900/4191 | Loss: 2.4452\n",
      "  Batch 2000/4191 | Loss: 3.1689\n",
      "  Batch 2100/4191 | Loss: 2.2673\n",
      "  Batch 2200/4191 | Loss: 3.0477\n",
      "  Batch 2300/4191 | Loss: 2.7257\n",
      "  Batch 2400/4191 | Loss: 2.6695\n",
      "  Batch 2500/4191 | Loss: 1.9180\n",
      "  Batch 2600/4191 | Loss: 2.8580\n",
      "  Batch 2700/4191 | Loss: 2.9632\n",
      "  Batch 2800/4191 | Loss: 2.5732\n",
      "  Batch 2900/4191 | Loss: 2.5721\n",
      "  Batch 3000/4191 | Loss: 2.3270\n",
      "  Batch 3100/4191 | Loss: 3.0481\n",
      "  Batch 3200/4191 | Loss: 2.2940\n",
      "  Batch 3300/4191 | Loss: 2.4366\n",
      "  Batch 3400/4191 | Loss: 2.8691\n",
      "  Batch 3500/4191 | Loss: 2.6378\n",
      "  Batch 3600/4191 | Loss: 3.1828\n",
      "  Batch 3700/4191 | Loss: 2.5750\n",
      "  Batch 3800/4191 | Loss: 2.5064\n",
      "  Batch 3900/4191 | Loss: 3.0121\n",
      "  Batch 4000/4191 | Loss: 2.1758\n",
      "  Batch 4100/4191 | Loss: 2.4207\n",
      "\n",
      "Epoch: 44 | Time: 4m 22s\n",
      "\tTrain Loss: 2.627 | Train PPL:  13.830\n",
      "\t Val. Loss: 3.607 |  Val. PPL:  36.843 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.3508\n",
      "  Batch 200/4191 | Loss: 2.2722\n",
      "  Batch 300/4191 | Loss: 2.1206\n",
      "  Batch 400/4191 | Loss: 2.8798\n",
      "  Batch 500/4191 | Loss: 2.4347\n",
      "  Batch 600/4191 | Loss: 2.7134\n",
      "  Batch 700/4191 | Loss: 2.5904\n",
      "  Batch 800/4191 | Loss: 2.2432\n",
      "  Batch 900/4191 | Loss: 2.6332\n",
      "  Batch 1000/4191 | Loss: 2.6309\n",
      "  Batch 1100/4191 | Loss: 2.6029\n",
      "  Batch 1200/4191 | Loss: 2.7603\n",
      "  Batch 1300/4191 | Loss: 2.2437\n",
      "  Batch 1400/4191 | Loss: 2.6739\n",
      "  Batch 1500/4191 | Loss: 2.5345\n",
      "  Batch 1600/4191 | Loss: 2.1953\n",
      "  Batch 1700/4191 | Loss: 2.6190\n",
      "  Batch 1800/4191 | Loss: 2.9899\n",
      "  Batch 1900/4191 | Loss: 2.7603\n",
      "  Batch 2000/4191 | Loss: 2.5360\n",
      "  Batch 2100/4191 | Loss: 2.4295\n",
      "  Batch 2200/4191 | Loss: 2.2932\n",
      "  Batch 2300/4191 | Loss: 2.5317\n",
      "  Batch 2400/4191 | Loss: 2.5569\n",
      "  Batch 2500/4191 | Loss: 2.3681\n",
      "  Batch 2600/4191 | Loss: 2.5911\n",
      "  Batch 2700/4191 | Loss: 3.2834\n",
      "  Batch 2800/4191 | Loss: 2.7658\n",
      "  Batch 2900/4191 | Loss: 2.3124\n",
      "  Batch 3000/4191 | Loss: 2.4635\n",
      "  Batch 3100/4191 | Loss: 2.7218\n",
      "  Batch 3200/4191 | Loss: 2.9045\n",
      "  Batch 3300/4191 | Loss: 3.0190\n",
      "  Batch 3400/4191 | Loss: 2.7037\n",
      "  Batch 3500/4191 | Loss: 2.7540\n",
      "  Batch 3600/4191 | Loss: 2.9075\n",
      "  Batch 3700/4191 | Loss: 2.8853\n",
      "  Batch 3800/4191 | Loss: 2.7074\n",
      "  Batch 3900/4191 | Loss: 2.8472\n",
      "  Batch 4000/4191 | Loss: 2.7879\n",
      "  Batch 4100/4191 | Loss: 2.5241\n",
      "\n",
      "Epoch: 45 | Time: 4m 29s\n",
      "\tTrain Loss: 2.596 | Train PPL:  13.410\n",
      "\t Val. Loss: 3.589 |  Val. PPL:  36.210 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.1816\n",
      "  Batch 200/4191 | Loss: 2.4910\n",
      "  Batch 300/4191 | Loss: 2.6314\n",
      "  Batch 400/4191 | Loss: 2.6417\n",
      "  Batch 500/4191 | Loss: 2.4906\n",
      "  Batch 600/4191 | Loss: 2.5617\n",
      "  Batch 700/4191 | Loss: 2.4602\n",
      "  Batch 800/4191 | Loss: 2.7580\n",
      "  Batch 900/4191 | Loss: 2.6085\n",
      "  Batch 1000/4191 | Loss: 2.3506\n",
      "  Batch 1100/4191 | Loss: 2.1505\n",
      "  Batch 1200/4191 | Loss: 2.5536\n",
      "  Batch 1300/4191 | Loss: 2.6000\n",
      "  Batch 1400/4191 | Loss: 2.3154\n",
      "  Batch 1500/4191 | Loss: 2.3824\n",
      "  Batch 1600/4191 | Loss: 2.7537\n",
      "  Batch 1700/4191 | Loss: 2.4663\n",
      "  Batch 1800/4191 | Loss: 2.4357\n",
      "  Batch 1900/4191 | Loss: 2.7414\n",
      "  Batch 2000/4191 | Loss: 2.5519\n",
      "  Batch 2100/4191 | Loss: 2.2749\n",
      "  Batch 2200/4191 | Loss: 2.5187\n",
      "  Batch 2300/4191 | Loss: 2.4422\n",
      "  Batch 2400/4191 | Loss: 2.4889\n",
      "  Batch 2500/4191 | Loss: 2.5074\n",
      "  Batch 2600/4191 | Loss: 2.7580\n",
      "  Batch 2700/4191 | Loss: 2.4850\n",
      "  Batch 2800/4191 | Loss: 2.8577\n",
      "  Batch 2900/4191 | Loss: 2.3441\n",
      "  Batch 3000/4191 | Loss: 2.8295\n",
      "  Batch 3100/4191 | Loss: 2.6144\n",
      "  Batch 3200/4191 | Loss: 2.5856\n",
      "  Batch 3300/4191 | Loss: 2.3286\n",
      "  Batch 3400/4191 | Loss: 3.0253\n",
      "  Batch 3500/4191 | Loss: 2.6068\n",
      "  Batch 3600/4191 | Loss: 2.5052\n",
      "  Batch 3700/4191 | Loss: 2.5884\n",
      "  Batch 3800/4191 | Loss: 2.8845\n",
      "  Batch 3900/4191 | Loss: 2.6035\n",
      "  Batch 4000/4191 | Loss: 3.0003\n",
      "  Batch 4100/4191 | Loss: 2.6555\n",
      "\n",
      "Epoch: 46 | Time: 4m 44s\n",
      "\tTrain Loss: 2.566 | Train PPL:  13.017\n",
      "\t Val. Loss: 3.579 |  Val. PPL:  35.826 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.6593\n",
      "  Batch 200/4191 | Loss: 2.8033\n",
      "  Batch 300/4191 | Loss: 2.3651\n",
      "  Batch 400/4191 | Loss: 2.5901\n",
      "  Batch 500/4191 | Loss: 2.6753\n",
      "  Batch 600/4191 | Loss: 2.4587\n",
      "  Batch 700/4191 | Loss: 2.3738\n",
      "  Batch 800/4191 | Loss: 2.3689\n",
      "  Batch 900/4191 | Loss: 2.3801\n",
      "  Batch 1000/4191 | Loss: 2.2322\n",
      "  Batch 1100/4191 | Loss: 2.3547\n",
      "  Batch 1200/4191 | Loss: 2.4784\n",
      "  Batch 1300/4191 | Loss: 2.5781\n",
      "  Batch 1400/4191 | Loss: 2.9882\n",
      "  Batch 1500/4191 | Loss: 3.0494\n",
      "  Batch 1600/4191 | Loss: 2.4403\n",
      "  Batch 1700/4191 | Loss: 2.4451\n",
      "  Batch 1800/4191 | Loss: 2.3729\n",
      "  Batch 1900/4191 | Loss: 2.4926\n",
      "  Batch 2000/4191 | Loss: 2.4836\n",
      "  Batch 2100/4191 | Loss: 2.9159\n",
      "  Batch 2200/4191 | Loss: 2.1902\n",
      "  Batch 2300/4191 | Loss: 2.5784\n",
      "  Batch 2400/4191 | Loss: 2.3861\n",
      "  Batch 2500/4191 | Loss: 2.9135\n",
      "  Batch 2600/4191 | Loss: 2.5078\n",
      "  Batch 2700/4191 | Loss: 2.2797\n",
      "  Batch 2800/4191 | Loss: 2.2101\n",
      "  Batch 2900/4191 | Loss: 3.0216\n",
      "  Batch 3000/4191 | Loss: 2.3779\n",
      "  Batch 3100/4191 | Loss: 2.8102\n",
      "  Batch 3200/4191 | Loss: 2.3995\n",
      "  Batch 3300/4191 | Loss: 2.6350\n",
      "  Batch 3400/4191 | Loss: 2.2116\n",
      "  Batch 3500/4191 | Loss: 2.6877\n",
      "  Batch 3600/4191 | Loss: 2.6030\n",
      "  Batch 3700/4191 | Loss: 2.7069\n",
      "  Batch 3800/4191 | Loss: 2.3619\n",
      "  Batch 3900/4191 | Loss: 2.6453\n",
      "  Batch 4000/4191 | Loss: 2.6392\n",
      "  Batch 4100/4191 | Loss: 2.7669\n",
      "\n",
      "Epoch: 47 | Time: 4m 48s\n",
      "\tTrain Loss: 2.538 | Train PPL:  12.656\n",
      "\t Val. Loss: 3.575 |  Val. PPL:  35.701 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.5316\n",
      "  Batch 200/4191 | Loss: 2.5451\n",
      "  Batch 300/4191 | Loss: 2.1661\n",
      "  Batch 400/4191 | Loss: 2.4183\n",
      "  Batch 500/4191 | Loss: 2.3572\n",
      "  Batch 600/4191 | Loss: 2.6378\n",
      "  Batch 700/4191 | Loss: 2.8082\n",
      "  Batch 800/4191 | Loss: 2.1235\n",
      "  Batch 900/4191 | Loss: 2.8044\n",
      "  Batch 1000/4191 | Loss: 2.1821\n",
      "  Batch 1100/4191 | Loss: 2.5879\n",
      "  Batch 1200/4191 | Loss: 2.6328\n",
      "  Batch 1300/4191 | Loss: 2.7040\n",
      "  Batch 1400/4191 | Loss: 2.5571\n",
      "  Batch 1500/4191 | Loss: 2.4315\n",
      "  Batch 1600/4191 | Loss: 2.4601\n",
      "  Batch 1700/4191 | Loss: 2.2079\n",
      "  Batch 1800/4191 | Loss: 2.5568\n",
      "  Batch 1900/4191 | Loss: 2.3029\n",
      "  Batch 2000/4191 | Loss: 2.7097\n",
      "  Batch 2100/4191 | Loss: 2.7798\n",
      "  Batch 2200/4191 | Loss: 2.7204\n",
      "  Batch 2300/4191 | Loss: 2.3853\n",
      "  Batch 2400/4191 | Loss: 2.6180\n",
      "  Batch 2500/4191 | Loss: 2.4490\n",
      "  Batch 2600/4191 | Loss: 2.7485\n",
      "  Batch 2700/4191 | Loss: 2.2598\n",
      "  Batch 2800/4191 | Loss: 2.3474\n",
      "  Batch 2900/4191 | Loss: 2.3769\n",
      "  Batch 3000/4191 | Loss: 2.4925\n",
      "  Batch 3100/4191 | Loss: 2.3848\n",
      "  Batch 3200/4191 | Loss: 2.4934\n",
      "  Batch 3300/4191 | Loss: 2.0520\n",
      "  Batch 3400/4191 | Loss: 2.5162\n",
      "  Batch 3500/4191 | Loss: 2.6979\n",
      "  Batch 3600/4191 | Loss: 2.3656\n",
      "  Batch 3700/4191 | Loss: 2.7040\n",
      "  Batch 3800/4191 | Loss: 2.8971\n",
      "  Batch 3900/4191 | Loss: 2.7076\n",
      "  Batch 4000/4191 | Loss: 2.5798\n",
      "  Batch 4100/4191 | Loss: 2.6217\n",
      "\n",
      "Epoch: 48 | Time: 4m 34s\n",
      "\tTrain Loss: 2.513 | Train PPL:  12.336\n",
      "\t Val. Loss: 3.555 |  Val. PPL:  34.974 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.5097\n",
      "  Batch 200/4191 | Loss: 2.3796\n",
      "  Batch 300/4191 | Loss: 2.3822\n",
      "  Batch 400/4191 | Loss: 2.5569\n",
      "  Batch 500/4191 | Loss: 2.3344\n",
      "  Batch 600/4191 | Loss: 2.7398\n",
      "  Batch 700/4191 | Loss: 2.6168\n",
      "  Batch 800/4191 | Loss: 2.7174\n",
      "  Batch 900/4191 | Loss: 2.3652\n",
      "  Batch 1000/4191 | Loss: 2.3889\n",
      "  Batch 1100/4191 | Loss: 2.4747\n",
      "  Batch 1200/4191 | Loss: 2.4988\n",
      "  Batch 1300/4191 | Loss: 2.6395\n",
      "  Batch 1400/4191 | Loss: 2.3002\n",
      "  Batch 1500/4191 | Loss: 2.3131\n",
      "  Batch 1600/4191 | Loss: 2.2273\n",
      "  Batch 1700/4191 | Loss: 2.2624\n",
      "  Batch 1800/4191 | Loss: 2.3595\n",
      "  Batch 1900/4191 | Loss: 2.6848\n",
      "  Batch 2000/4191 | Loss: 3.1347\n",
      "  Batch 2100/4191 | Loss: 2.6998\n",
      "  Batch 2200/4191 | Loss: 2.6635\n",
      "  Batch 2300/4191 | Loss: 2.5902\n",
      "  Batch 2400/4191 | Loss: 2.6255\n",
      "  Batch 2500/4191 | Loss: 2.8789\n",
      "  Batch 2600/4191 | Loss: 2.5012\n",
      "  Batch 2700/4191 | Loss: 2.6065\n",
      "  Batch 2800/4191 | Loss: 2.3101\n",
      "  Batch 2900/4191 | Loss: 2.5981\n",
      "  Batch 3000/4191 | Loss: 2.3118\n",
      "  Batch 3100/4191 | Loss: 2.6731\n",
      "  Batch 3200/4191 | Loss: 2.5010\n",
      "  Batch 3300/4191 | Loss: 2.5801\n",
      "  Batch 3400/4191 | Loss: 2.6612\n",
      "  Batch 3500/4191 | Loss: 2.6317\n",
      "  Batch 3600/4191 | Loss: 2.3489\n",
      "  Batch 3700/4191 | Loss: 2.3254\n",
      "  Batch 3800/4191 | Loss: 2.4947\n",
      "  Batch 3900/4191 | Loss: 2.8350\n",
      "  Batch 4000/4191 | Loss: 2.4942\n",
      "  Batch 4100/4191 | Loss: 2.6284\n",
      "\n",
      "Epoch: 49 | Time: 4m 23s\n",
      "\tTrain Loss: 2.478 | Train PPL:  11.918\n",
      "\t Val. Loss: 3.542 |  Val. PPL:  34.526 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "  Batch 100/4191 | Loss: 2.7088\n",
      "  Batch 200/4191 | Loss: 2.1291\n",
      "  Batch 300/4191 | Loss: 2.3227\n",
      "  Batch 400/4191 | Loss: 2.5444\n",
      "  Batch 500/4191 | Loss: 2.4006\n",
      "  Batch 600/4191 | Loss: 2.6317\n",
      "  Batch 700/4191 | Loss: 2.7047\n",
      "  Batch 800/4191 | Loss: 2.3346\n",
      "  Batch 900/4191 | Loss: 2.1868\n",
      "  Batch 1000/4191 | Loss: 2.6417\n",
      "  Batch 1100/4191 | Loss: 2.4197\n",
      "  Batch 1200/4191 | Loss: 2.2041\n",
      "  Batch 1300/4191 | Loss: 2.6034\n",
      "  Batch 1400/4191 | Loss: 2.3684\n",
      "  Batch 1500/4191 | Loss: 2.3126\n",
      "  Batch 1600/4191 | Loss: 2.4931\n",
      "  Batch 1700/4191 | Loss: 2.5600\n",
      "  Batch 1800/4191 | Loss: 2.5837\n",
      "  Batch 1900/4191 | Loss: 2.5463\n",
      "  Batch 2000/4191 | Loss: 2.2945\n",
      "  Batch 2100/4191 | Loss: 2.4789\n",
      "  Batch 2200/4191 | Loss: 2.4760\n",
      "  Batch 2300/4191 | Loss: 2.6548\n",
      "  Batch 2400/4191 | Loss: 2.2574\n",
      "  Batch 2500/4191 | Loss: 2.2494\n",
      "  Batch 2600/4191 | Loss: 2.5943\n",
      "  Batch 2700/4191 | Loss: 2.0730\n",
      "  Batch 2800/4191 | Loss: 2.4626\n",
      "  Batch 2900/4191 | Loss: 2.4913\n",
      "  Batch 3000/4191 | Loss: 2.5742\n",
      "  Batch 3100/4191 | Loss: 2.3543\n",
      "  Batch 3200/4191 | Loss: 2.4611\n",
      "  Batch 3300/4191 | Loss: 2.6664\n",
      "  Batch 3400/4191 | Loss: 2.3095\n",
      "  Batch 3500/4191 | Loss: 2.1648\n",
      "  Batch 3600/4191 | Loss: 2.3296\n",
      "  Batch 3700/4191 | Loss: 2.4261\n",
      "  Batch 3800/4191 | Loss: 2.4685\n",
      "  Batch 3900/4191 | Loss: 2.4937\n",
      "  Batch 4000/4191 | Loss: 2.2786\n",
      "  Batch 4100/4191 | Loss: 2.8941\n",
      "\n",
      "Epoch: 50 | Time: 4m 24s\n",
      "\tTrain Loss: 2.450 | Train PPL:  11.585\n",
      "\t Val. Loss: 3.529 |  Val. PPL:  34.099 | ✅ Model Saved! (New Best)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Training Complete!\n",
      "Best Validation Loss: 3.529\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "print(\"Saving initial model weights...\")\n",
    "torch.save(model.state_dict(), 'models/model_init.pt')\n",
    "print(\"✅ Initial model saved to 'models/model_init.pt'\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Training...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    \n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "        save_msg = \"✅ Model Saved! (New Best)\"\n",
    "    else:\n",
    "        save_msg = \"\"\n",
    "    \n",
    "    print(f'\\nEpoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | {save_msg}')\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Validation Loss: {best_valid_loss:.3f}\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Complete checkpoint saved to 'nmt-model-checkpoint.pth'\n",
      "   (Includes Model, Optimizer, and Hyperparameters)\n",
      "   Ready to resume from Epoch 50\n"
     ]
    }
   ],
   "source": [
    "checkpoint = {\n",
    "    'epoch': N_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': best_valid_loss,\n",
    "    'hyperparameters': {\n",
    "        'input_dim': INPUT_DIM,\n",
    "        'output_dim': OUTPUT_DIM,\n",
    "        'hid_dim': HID_DIM,\n",
    "        'enc_layers': ENC_LAYERS,\n",
    "        'dec_layers': DEC_LAYERS,\n",
    "        'enc_heads': ENC_HEADS,\n",
    "        'dec_heads': DEC_HEADS,\n",
    "        'enc_dropout': ENC_DROPOUT,\n",
    "        'dec_dropout': DEC_DROPOUT,\n",
    "        'device': str(device)\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'nmt-model-checkpoint.pth')\n",
    "\n",
    "print(\"✅ Complete checkpoint saved to 'nmt-model-checkpoint.pth'\")\n",
    "print(\"   (Includes Model, Optimizer, and Hyperparameters)\")\n",
    "print(f\"   Ready to resume from Epoch {N_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "490f3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Translation Test ---\n",
      "English: Hello how are you?\n",
      "Nepali:  नमस्कार, तपाईं कसरी हुनुहुन्छ?\n",
      "------------------------------\n",
      "English: I am going home.\n",
      "Nepali:  म घर जाँदैछु ।\n",
      "------------------------------\n",
      "English: This is a book.\n",
      "Nepali:  यो पुस्तक हो ।\n",
      "------------------------------\n",
      "English: He loves to play football.\n",
      "Nepali:  उसले एउटा सानो टुक्रामा लग्यो ।\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load('best-model.pt'))\n",
    "model.eval() \n",
    "\n",
    "def translate_sentence(sentence, model, device, max_len=50):\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [1] + sp_en.encode_as_ids(sentence) + [2]\n",
    "    else:\n",
    "        tokens = [1] + sentence + [2]\n",
    "        \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device) \n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indices = [1] \n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indices.append(pred_token)\n",
    "        \n",
    "        if pred_token == 2:\n",
    "            break\n",
    "            \n",
    "    trg_tokens = [t for t in trg_indices if t not in [1, 2]]\n",
    "    translated_text = sp_ne.decode_ids(trg_tokens)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "print(\"\\n--- Translation Test ---\")\n",
    "\n",
    "sentences = [\n",
    "    \"Hello how are you?\",\n",
    "    \"I am going home.\",\n",
    "    \"This is a book.\",\n",
    "    \"He loves to play football.\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    translation = translate_sentence(s, model, device)\n",
    "    print(f\"English: {s}\")\n",
    "    print(f\"Nepali:  {translation}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5dade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
